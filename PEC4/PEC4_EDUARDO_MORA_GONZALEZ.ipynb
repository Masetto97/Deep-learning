{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.975 · Deep Learning · PEC4</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Master universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informatica, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC 4: Modelos generativos\n",
    "\n",
    "En esta práctica implementaremos uno de los tipos de modelos generativos más utilizados actualmente, las redes generativas adversarias, ie. **GANs**.\n",
    "\n",
    "<u>Consideraciones generales</u>:\n",
    "\n",
    "- Esta PEC debe realizarse de manera **estrictamente individual**. Cualquier indicio de copia será penalizado con un suspenso (D) para todas las partes implicadas y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "- Es necesario que el estudiante indique **todas las fuentes** que ha utilizado para la realización de la PEC. Si no es así, se considerará que el estudiante ha cometido plagio, siendo penalizado con un suspenso (D) y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "\n",
    "<u>Formato de entrega</u>:\n",
    "\n",
    "- Algunos ejercicios pueden suponer varios minutos de ejecución, por lo que la entrega debe realizarse en **formato notebook** y en **formato html**, donde se vea el código, los resultados y comentarios de cada ejercicio. Se puede exportar el notebook a HTML desde el menú File $\\to$ Download as $\\to$ HTML.\n",
    "- Existe un tipo de celda especial para albergar texto. Este tipo de celda le será muy útil para responder a las diferentes preguntas teóricas planteadas a lo largo de la actividad. Puede cambiar el tipo de celda a este tipo, en el menú: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introducción\n",
    "\n",
    "El objetivo de esta PEC es comprender la implementación de una solución generativa, utilizando DCGANs para la generación de imágenes, mediante el conjunto de datos de referencia en deep learning más sencillo existente: MNIST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T09:22:58.686067Z",
     "iopub.status.busy": "2023-06-02T09:22:58.685350Z",
     "iopub.status.idle": "2023-06-02T09:22:58.692605Z",
     "shell.execute_reply": "2023-06-02T09:22:58.691735Z",
     "shell.execute_reply.started": "2023-06-02T09:22:58.686033Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obtención de los datos\n",
    "\n",
    "El código para cargar los datos es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T09:23:43.440465Z",
     "iopub.status.busy": "2023-06-02T09:23:43.439991Z",
     "iopub.status.idle": "2023-06-02T09:23:44.832815Z",
     "shell.execute_reply": "2023-06-02T09:23:44.831555Z",
     "shell.execute_reply.started": "2023-06-02T09:23:43.440431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "img_channels = 1\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1,5 pts.]:</strong> \n",
    "Añade un comentario explicativo, a cada una de las líneas de código de abajo, indicando cuál es su funcionalidad.</div>\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "* `latent_dim = 100`: Esta línea de código asigna el valor 100 a la variable `latent_dim`. Esta variable representa la dimensión del espacio latente en un modelo de autoencoder o generador. El espacio latente es el espacio de representación de los vectores de entrada que se utilizan para generar imágenes.\n",
    "\n",
    "* `img_rows, img_cols = 28, 28`: Esta línea de código asigna los valores 28 a las variables `img_rows` e `img_cols`. Estas variables representan el número de filas y columnas de las imágenes en el conjunto de datos. Estas variables se utilizan para redimensionar las imágenes en el conjunto de datos `x_train` a un tamaño de 28x28 píxeles.  \n",
    "\n",
    "* `img_channels = 1`: Esta línea de código asigna el valor 1 a la variable `img_channels`. Esta variable representa el número de canales de color de las imágenes en el conjunto de datos. En este caso, como el conjunto de datos es `MNIST`, las imágenes son en escala de grises, por lo que solo hay un canal de color.\n",
    "\n",
    "* `(x_train, _), (_, _) = mnist.load_data()`: Esta línea de código carga los datos del conjunto de datos `MNIST`. La función `mnist.load_data()` devuelve dos tuplas, la primera contiene el conjunto de entrenamiento (`x_train`) y sus etiquetas correspondientes (`y_train`), y la segunda contiene el conjunto de prueba (x_test) y sus etiquetas correspondientes (y_test). En este caso, solo estamos interesados en el conjunto de entrenamiento, por lo que se utiliza el guion bajo para descartar las etiquetas.\n",
    "\n",
    "* `x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)`: Esta línea de código reorganiza los datos de entrenamiento para que tengan la forma adecuada para el modelo. Utiliza la función `reshape` para cambiar la forma de `x_train` a (número de muestras, filas, columnas, canales). En este caso, estamos ajustando las imágenes a la forma (número de muestras, 28, 28, 1).\n",
    "\n",
    "* `x_train = x_train.astype('float32')`: Esta línea de código se utiliza para convertir los elementos del conjunto de entrenamiento x_train al tipo de datos `float32`. Esto se hace mediante el método astype y especificando el tipo de datos deseado como argumento. La conversión a `float32` es común en tareas de procesamiento de imágenes, ya que permite un cálculo preciso y eficiente de operaciones matemáticas en los datos.\n",
    "\n",
    "* `x_train /= 255`: Esta línea de código se utiliza para normalizar el conjunto de entrenamiento x_train dividiendo todos los valores de píxeles por 255. Esto se realiza para escalar los valores de píxeles de 0 a 255 a un rango de 0 a 1. La normalización ayuda a mejorar el rendimiento del modelo y facilita el proceso de entrenamiento al tener valores en un rango más manejable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementación del Generador\n",
    "\n",
    "A continuación se muestra una propuesta de generador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:08.008808Z",
     "iopub.status.busy": "2023-05-31T18:23:08.008441Z",
     "iopub.status.idle": "2023-05-31T18:23:08.020678Z",
     "shell.execute_reply": "2023-05-31T18:23:08.019819Z",
     "shell.execute_reply.started": "2023-05-31T18:23:08.008777Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_model(): \n",
    "    dropout = 0.4\n",
    "    depth = 256 # 64+64+64+64\n",
    "    dim = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 100\n",
    "    # Out: dim x dim x depth\n",
    "    model.add(Dense(dim*dim*depth, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((dim, dim, depth)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # In: dim x dim x depth\n",
    "    # Out: 2*dim x 2*dim x depth/2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1,75 pts.]:</strong> \n",
    "Contesta a las preguntas siguientes:\n",
    "</div>\n",
    "\n",
    "**1. ¿Cuál es la finalidad del generador?:**\n",
    "\n",
    "La finalidad del generador es crear imágenes sintéticas a partir de un espacio latente. En el contexto de las redes generativas adversarias (GANs), el generador es una parte del modelo que genera nuevas muestras que intentan ser indistinguibles de las muestras reales en el conjunto de datos de entrenamiento. El objetivo del generador es aprender a mapear puntos en el espacio latente a imágenes realistas. Es decir, el generador toma una muestra de ruido aleatoria y la transforma en una imagen que se parece a las imágenes reales del conjunto de datos.\n",
    "\n",
    "**2. Investigar por qué se utiliza `Upsampling` en las dos primeras capas en lugar de la `Conv2DTranspose` propuesta en DCGAN. Dar una justificación:**\n",
    "\n",
    "En lugar de utilizar solo capas de `Conv2DTranspose`, se utiliza `Upsampling` seguido de `Conv2DTranspose` en las dos primeras capas. Esto se debe a que Upsampling permite aumentar el tamaño de la entrada mediante una interpolación bilineal, lo que puede preservar mejor la estructura y los detalles de la imagen original. Luego, la `Conv2DTranspose` se utiliza para realizar la convolución transpuesta y ajustar los detalles finos de la imagen generada. Esta combinación de `Upsampling` y `Conv2DTranspose` puede ayudar a generar imágenes más nítidas y realistas.\n",
    "\n",
    "**3. ¿Por qué se utiliza la normalización entre capas?**\n",
    "\n",
    "Se utiliza la normalización entre capas para estabilizar y acelerar el proceso de entrenamiento de la red neuronal. La normalización ayuda a reducir la covariación de las activaciones de las capas anteriores, lo que evita que los valores se disparen o colapsen en rangos extremos. Esto es especialmente útil en modelos profundos como GANs, donde las capas están interconectadas y pueden sufrir de inestabilidad durante el entrenamiento.\n",
    "\n",
    "**4. ¿Qué funciones de activación se utilizan? ¿Cuál es la razón de la sigmoide en la última capa?**\n",
    "\n",
    "En este modelo, se utiliza la función de activación `ReLU (Rectified Linear Unit)` en la mayoría de las capas, excepto en la última capa donde se utiliza la función de activación `sigmoide`. \n",
    "\n",
    "   - La `ReLU` se utiliza comúnmente en las capas intermedias para introducir no linealidad en la red y permitir la representación de características más complejas. \n",
    "   - La `sigmoide` se utiliza en la última capa para asegurarse de que los valores de los píxeles de la imagen generada estén en el rango de [0, 1], lo que es adecuado para imágenes en escala de grises donde los píxeles se interpretan como valores de intensidad. La sigmoide comprime los valores a un rango válido y garantiza que la imagen generada sea una imagen de escala de grises que se asemeje a las reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación del Discriminador\n",
    "\n",
    "A continuación se muestra el discriminador propuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:22:22.594199Z",
     "iopub.status.busy": "2023-05-31T18:22:22.593849Z",
     "iopub.status.idle": "2023-05-31T18:22:22.603257Z",
     "shell.execute_reply": "2023-05-31T18:22:22.602395Z",
     "shell.execute_reply.started": "2023-05-31T18:22:22.594170Z"
    }
   },
   "outputs": [],
   "source": [
    "# (W−F+2P)/S+1\n",
    "def discriminator_model():\n",
    "    depth = 64\n",
    "    dropout = 0.4\n",
    "    input_shape = (img_rows, img_cols, img_channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 28 x 28 x 1, depth = 1\n",
    "    # Out: 14 x 14 x 1, depth=64\n",
    "    model.add(Conv2D(depth, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Out: 1-dim probability\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1,75 pts.]:</strong> \n",
    "Contesta a las preguntas siguientes:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Cuál es la finalidad del discriminador?:**\n",
    "\n",
    "La finalidad del discriminador es aprender a clasificar correctamente las muestras como \"real\" o \"falsa\" (generada). El discriminador se entrena con un conjunto de datos que contiene muestras reales y muestras generadas por el generador. A medida que el discriminador se entrena, se vuelve más hábil en la discriminación y su capacidad para distinguir entre las dos clases mejora. En decir, la finalidad del discriminador es actuar como un \"juez\" que evalúa la autenticidad de las muestras generadas por el generador en una GAN.\n",
    " \n",
    "**2. ¿Cuáles son las dimensiones de los tensores y características de las variables de entrada y salida del discriminador? :**\n",
    "\n",
    "Las dimensiones de los tensores de entrada del discriminador son (28, 28, 1), que corresponde al tamaño de una imagen del conjunto de datos `MNIST`. El discriminador espera recibir imágenes en escala de grises de 28x28 píxeles. La dimensión de los tensores de salida es un solo número, ya que el discriminador produce una sola probabilidad para cada imagen de entrada, que indica la probabilidad de ser una imagen real.\n",
    "\n",
    "**3. ¿Cuál es la diferencia con una CNN habitual?**\n",
    "\n",
    "La diferencia con una CNN habitual radica en que el discriminador se ha diseñado específicamente para la tarea de distinguir entre imágenes reales y generadas por el generador en el contexto de una GAN. Utiliza varias capas convolucionales para extraer características de las imágenes y luego las aplana en una representación unidimensional antes de pasarlas a una capa densa final. Además, se utilizan funciones de activación y técnicas como el dropout para mejorar el rendimiento del discriminador en esta tarea específica.\n",
    "\n",
    "**4. ¿Qué funciones de activación se utilizan?**\n",
    "\n",
    "En el discriminador se utilizan las funciones de activación `LeakyReLU` y `sigmoid`: \n",
    "\n",
    "   - `LeakyReLU` se utiliza en las capas convolucionales para introducir no linealidad y evitar el problema de la \"unidad muerta o desvanecimiento de gradientes\" que puede ocurrir con `ReLU` estándar.\n",
    "\n",
    "   - `Sigmoid` se utiliza en la capa densa final para generar una probabilidad en el rango de 0 a 1, donde valores cercanos a 0 indican una alta probabilidad de ser una imagen generada y valores cercanos a 1 indican una alta probabilidad de ser una imagen real.\n",
    "\n",
    "**5. ¿Cuál es la finalidad del dropout que encontramos en las capas?**\n",
    "\n",
    "El dropout se utiliza como una técnica de regularización en las capas del discriminador. Al desactivar aleatoriamente una fracción de las unidades de salida durante el entrenamiento, el dropout evita la dependencia excesiva de unidades específicas y ayuda a prevenir el sobreajuste. Esto mejora la capacidad de generalización del modelo y ayuda a evitar que el discriminador se ajuste demasiado a los datos de entrenamiento específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelo GAN\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. A qué llamamos modelo GAN y por qué recibe ese nombre?:**\n",
    "\n",
    "Un modelo GAN (Generative Adversarial Network, en inglés) es un tipo de arquitectura de red neuronal que consiste en dos componentes principales: un generador y un discriminador. El nombre GAN proviene de la interacción adversarial entre estos dos componentes durante el entrenamiento.\n",
    "\n",
    "Como se ha mencionado anteriormente`el generador` se encarga de crear muestras sintéticas, como imágenes, a partir de un espacio latente aleatorio. Su objetivo es generar muestras que sean lo más similares posible a las muestras reales del conjunto de datos. Por otro lado,`el discriminador` por otro lado, es un clasificador binario que intenta distinguir entre muestras reales y muestras generadas por el generador. Su objetivo es aprender a discriminar correctamente entre las dos clases.\n",
    "\n",
    "Durante el entrenamiento, el generador y el discriminador se actualizan de forma adversarial. El generador busca mejorar su capacidad para engañar al discriminador generando muestras más realistas, mientras que el discriminador busca mejorar su capacidad para distinguir entre muestras reales y generadas. Este proceso de retroalimentación entre el generador y el discriminador se realiza de manera iterativa hasta que se logre un equilibrio o convergencia en el que el generador sea capaz de generar muestras indistinguibles de las reales y el discriminador ya no pueda distinguirlas con certeza.\n",
    "\n",
    "En resumen, el modelo GAN se llama así debido a la adversarialidad y la competencia entre el generador y el discriminador durante el entrenamiento, lo que impulsa la mejora y el aprendizaje mutuo de ambos componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Modelo Discriminador\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Qué función de pérdida utiliza el discriminador? ¿Por qué? :**\n",
    "\n",
    "El discriminador utiliza una función de pérdida basada en entropía cruzada binaria (`binary cross-entropy`) para su entrenamiento. Esta función de pérdida compara la salida del discriminador, que representa la probabilidad de que una imagen sea real, con las etiquetas de clase correspondientes (0 para imágenes generadas y 1 para imágenes reales). La entropía cruzada binaria es una elección común para problemas de clasificación binaria, ya que penaliza de manera efectiva las discrepancias entre las predicciones del modelo y las etiquetas verdaderas, incentivando al discriminador a asignar probabilidades cercanas a 0 para las imágenes generadas y cercanas a 1 para las imágenes reales.\n",
    "\n",
    "**2. Busca en la bibliografía la razón por la que se propone utilizar `RMSProp` como optimizador en vez de otros.**\n",
    "\n",
    "`RMSProp (Root Mean Square Propagation)` es un algoritmo de optimización que se utiliza comúnmente en modelos GAN debido a su capacidad para adaptar la tasa de aprendizaje de forma adaptativa para cada parámetro de la red neuronal. A diferencia de otros optimizadores como el `Descenso de Gradiente Estocástico (SGD)`, que utiliza una tasa de aprendizaje fija, `RMSProp` ajusta la tasa de aprendizaje de forma individual para cada parámetro según la magnitud de los gradientes previos. Esto permite una convergencia más rápida y estable en el entrenamiento de los modelos GAN, ya que se adapta a diferentes escalas y características de los gradientes en los distintos parámetros de la red\n",
    "\n",
    "**3. ¿Cuál es la razón de utilizar decay?**\n",
    "\n",
    "El parámetro de `decay` (decaimiento) se utiliza en el contexto del optimizador `RMSProp` para reducir gradualmente la tasa de aprendizaje a medida que avanza el entrenamiento. Esto es beneficioso para mejorar la estabilidad y la convergencia del modelo. El decaimiento de la tasa de aprendizaje ayuda a evitar que el modelo se estanque en mínimos locales subóptimos y permite explorar el espacio de búsqueda de manera más efectiva. Reducir la tasa de aprendizaje a lo largo del entrenamiento puede ayudar al modelo a converger hacia soluciones mejores y más estables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:18.573063Z",
     "iopub.status.busy": "2023-05-31T18:23:18.572413Z",
     "iopub.status.idle": "2023-05-31T18:23:18.666145Z",
     "shell.execute_reply": "2023-05-31T18:23:18.665235Z",
     "shell.execute_reply.started": "2023-05-31T18:23:18.573028Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()\n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "                      optimizer=RMSprop(lr=0.0002, decay=6e-8), \n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:20.651081Z",
     "iopub.status.busy": "2023-05-31T18:23:20.650741Z",
     "iopub.status.idle": "2023-05-31T18:23:20.843043Z",
     "shell.execute_reply": "2023-05-31T18:23:20.842148Z",
     "shell.execute_reply.started": "2023-05-31T18:23:20.651051Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Modelo adversario\n",
    "\n",
    "El modelo adversario es únicamente el generador-discriminador apilados juntos. Los parámetros de entrenamiento son los mismos que en el modelo Discriminador, salvo por una tasa de aprendizaje reducida y la correspondiente disminución del peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:22.905718Z",
     "iopub.status.busy": "2023-05-31T18:23:22.905334Z",
     "iopub.status.idle": "2023-05-31T18:23:22.911343Z",
     "shell.execute_reply": "2023-05-31T18:23:22.910382Z",
     "shell.execute_reply.started": "2023-05-31T18:23:22.905685Z"
    }
   },
   "outputs": [],
   "source": [
    "def adversarial_model():\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=RMSprop(lr=0.0001, decay=3e-8), \n",
    "                  metrics=['accuracy'])\n",
    "    discriminator.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:24.540924Z",
     "iopub.status.busy": "2023-05-31T18:23:24.540586Z",
     "iopub.status.idle": "2023-05-31T18:23:24.694834Z",
     "shell.execute_reply": "2023-05-31T18:23:24.692764Z",
     "shell.execute_reply.started": "2023-05-31T18:23:24.540894Z"
    }
   },
   "outputs": [],
   "source": [
    "adversarial = adversarial_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:34.145361Z",
     "iopub.status.busy": "2023-05-31T18:23:34.144378Z",
     "iopub.status.idle": "2023-05-31T18:23:34.155002Z",
     "shell.execute_reply": "2023-05-31T18:23:34.153713Z",
     "shell.execute_reply.started": "2023-05-31T18:23:34.145316Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(saveToFile=False, fake=True, samples=16, noise=None, epoch=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, latent_dim])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % epoch\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [img_rows, img_cols])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if saveToFile:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero determinamos si el modelo de discriminador es correcto entrenándolo solo con imágenes reales y falsas. Después, los modelos Discriminador y Adversario entrenan uno tras otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:23:36.750729Z",
     "iopub.status.busy": "2023-05-31T18:23:36.750356Z",
     "iopub.status.idle": "2023-05-31T18:23:36.762282Z",
     "shell.execute_reply": "2023-05-31T18:23:36.761256Z",
     "shell.execute_reply.started": "2023-05-31T18:23:36.750698Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_epochs=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_dim])\n",
    "        for epoch in range(train_epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # select a random half of images\n",
    "            images_train = x_train[np.random.randint(0, x_train.shape[0], size=batch_size), :, :, :]\n",
    "            \n",
    "            # sample noise and generate a batch of new images\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
    "            images_fake = generator.predict(noise)\n",
    "            \n",
    "            # train the discriminator (real classified as ones and generated as zeros)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = discriminator.train_on_batch(x, y)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            \n",
    "            # train the generator (wants discriminator to mistake images as real)\n",
    "            y = np.ones([batch_size, 1])\n",
    "            a_loss = adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            log_msg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "            log_msg = \"%s  [A loss: %f, acc: %f]\" % (log_msg, a_loss[0], a_loss[1])\n",
    "            print(log_msg)\n",
    "            if save_interval>0:\n",
    "                if (epoch+1)%save_interval==0:\n",
    "                    plot_images(saveToFile=True, samples=noise_input.shape[0],\n",
    "                                noise=noise_input, epoch=(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [2 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Cuál es la finalidad de `noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])`? ¿Por qué estas dimensiones?**\n",
    "\n",
    "La variable `noise` se utiliza para generar ruido latente aleatorio, que se utilizará como entrada para el generador durante el entrenamiento. \n",
    "\n",
    "El ruido latente es un vector de números aleatorios que se muestrea de una distribución uniforme en el rango de -1.0 a 1.0. La dimensión del ruido latente es determinada por `size=[batch_size, latent_dim]`, donde `batch_size` es el número de muestras de ruido latente generadas y `latent_dim` es la dimensión del espacio latente.\n",
    "\n",
    "La finalidad de generar ruido latente aleatorio es proporcionar una entrada aleatoria para el generador en cada época de entrenamiento. Esto permite al generador aprender a mapear diferentes regiones del espacio latente a imágenes diferentes. Al generar ruido latente aleatorio, se pueden explorar diferentes posiciones en el espacio latente y generar una variedad de imágenes diferentes en cada época de entrenamiento.\n",
    "\n",
    "\n",
    "**2. ¿Cuál es la finalidad de `images_fake = generator.predict(noise)`?**\n",
    "\n",
    "La línea de código anterior se utiliza para generar imágenes falsas utilizando el generador del modelo GAN. Toma el ruido latente noise como entrada y genera imágenes sintéticas.\n",
    "\n",
    "El generador toma el ruido latente como entrada y lo mapea a un espacio de características que representa una imagen. A medida que el modelo GAN se entrena, el generador aprende a generar imágenes que se asemejan a las del conjunto de datos original.\n",
    "\n",
    "La finalidad de generar imágenes falsas es proporcionar una comparación para el discriminador durante el entrenamiento. El discriminador intentará distinguir entre las imágenes reales y las imágenes falsas generadas por el generador. El generador es entrenado para engañar al discriminador al generar imágenes que se asemejan a las imágenes reales tanto como sea posible.\n",
    "\n",
    "Generar imágenes falsas permite evaluar la capacidad del generador para generar imágenes creíbles y realistas. Además, proporciona una retroalimentación para el ajuste de los pesos del generador durante el entrenamiento, ya que se utiliza en el cálculo de la pérdida del modelo GAN y la actualización de los pesos del generador.\n",
    "\n",
    "**3. ¿Cuál es la finalidad del código que sigue?**\n",
    "```python\n",
    "x = np.concatenate((images_train, images_fake))\n",
    "y = np.ondas([2*batch_size, 1])\n",
    "y[batch_size:, :] = 0\n",
    "```\n",
    "\n",
    "El código tiene la finalidad de preparar los datos para entrenar el discriminador en un paso de entrenamiento del modelo GAN.\n",
    "\n",
    "En primer lugar, se utiliza la función np.concatenate() para combinar las imágenes reales (images_train) y las imágenes falsas generadas por el generador (images_fake) en un solo tensor x. El resultado es un tensor que contiene tanto las imágenes reales como las imágenes falsas en un mismo conjunto de datos.\n",
    "\n",
    "A continuación, se crea el tensor de etiquetas y utilizando la función np.ondas() para inicializarlo con valores de 1. La dimensión del tensor de etiquetas es `[2*batch_size, 1]`, donde batch_size es el tamaño del lote utilizado en el entrenamiento. Esto significa que las primeras `batch_size` filas de y se establecerán en 1, indicando que son imágenes reales, mientras que las filas restantes se inicializarán en 0, indicando que son imágenes falsas.\n",
    "\n",
    "El propósito de esto es proporcionar etiquetas adecuadas para el entrenamiento del discriminador. Durante el entrenamiento, el discriminador se entrena para clasificar correctamente las imágenes como reales (1) o falsas (0). Al combinar las imágenes reales y falsas con sus respectivas etiquetas en un solo conjunto de datos x e y, el discriminador puede aprender a distinguir entre ellas y ajustar sus pesos en consecuencia durante el entrenamiento.\n",
    "\n",
    "**4. ¿Qué realiza el comando `d_loss = discriminator.train_on_batch(x, y)`? ¿Qué devuelve?**\n",
    "\n",
    "El comando `d_loss = discriminator.train_on_batch(x, y)` ejecuta el paso de entrenamiento del discriminador con el lote de datos proporcionado. Actualiza los pesos del discriminador en función de la diferencia entre las predicciones del discriminador y las etiquetas reales, utilizando la función de pérdida definida en el modelo del discriminador.\n",
    "\n",
    "El valor devuelto por `train_on_batch` es `d_loss`, que contiene la pérdida del discriminador en el lote de datos proporcionado. En este caso, se utiliza la métrica de pérdida binaria cruzada (binary_crossentropy) y la métrica de precisión (accuracy) al compilar el modelo del discriminador, por lo que d_loss contendrá la pérdida y la precisión calculadas durante el entrenamiento.\n",
    "\n",
    "**5. ¿Qué realiza el comando `a_loss = adversarial.train_on_batch(noise, y)`? ¿Qué devuelve?**\n",
    "\n",
    "El comando ejecuta el paso de entrenamiento del modelo adversarial con el lote de datos proporcionado. Primero, el generador genera imágenes falsas utilizando el ruido latente. Luego, el modelo adversarial intenta engañar al discriminador para que clasifique las imágenes falsas generadas como reales. El generador se entrena para que las imágenes generadas se asemejen a las imágenes reales tanto como sea posible.\n",
    "\n",
    "Durante el entrenamiento, los pesos del generador se actualizan en función de la diferencia entre las predicciones del discriminador y las etiquetas reales (y), utilizando la función de pérdida definida en el modelo adversarial.\n",
    "\n",
    "El valor devuelto por `train_on_batch es a_loss`, que contiene la pérdida del modelo adversarial en el lote de datos proporcionado. En este caso, se utiliza la métrica de pérdida binaria cruzada (binary_crossentropy) y la métrica de precisión (accuracy) al compilar el modelo adversarial, por lo que a_loss contendrá la pérdida y la precisión calculadas durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:32:14.556528Z",
     "iopub.status.busy": "2023-05-31T18:32:14.556163Z",
     "iopub.status.idle": "2023-05-31T18:32:14.563358Z",
     "shell.execute_reply": "2023-05-31T18:32:14.562262Z",
     "shell.execute_reply.started": "2023-05-31T18:32:14.556489Z"
    }
   },
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-31T18:32:24.218708Z",
     "iopub.status.busy": "2023-05-31T18:32:24.218344Z",
     "iopub.status.idle": "2023-05-31T18:36:16.074291Z",
     "shell.execute_reply": "2023-05-31T18:36:16.073350Z",
     "shell.execute_reply.started": "2023-05-31T18:32:24.218678Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 18:32:31.213486: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/dropout_8/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-31 18:32:33.946715: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_5/sequential_3/dropout_8/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.696192, acc: 0.457031]  [A loss: 0.976893, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "1: [D loss: 0.604550, acc: 0.716797]  [A loss: 2.024027, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "2: [D loss: 0.371413, acc: 0.943359]  [A loss: 0.015334, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "3: [D loss: 0.827459, acc: 0.500000]  [A loss: 4.366349, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "4: [D loss: 0.367373, acc: 0.824219]  [A loss: 1.080720, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "5: [D loss: 0.196816, acc: 0.996094]  [A loss: 0.809894, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "6: [D loss: 0.153330, acc: 0.984375]  [A loss: 0.511204, acc: 0.781250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "7: [D loss: 0.124745, acc: 0.988281]  [A loss: 0.238091, acc: 0.976562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "8: [D loss: 0.094832, acc: 0.988281]  [A loss: 0.160770, acc: 0.988281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "9: [D loss: 0.073156, acc: 0.996094]  [A loss: 0.111430, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "10: [D loss: 0.062814, acc: 0.998047]  [A loss: 0.079253, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "11: [D loss: 0.048954, acc: 1.000000]  [A loss: 0.091970, acc: 0.984375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "12: [D loss: 0.048946, acc: 0.994141]  [A loss: 0.044231, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "13: [D loss: 0.037501, acc: 0.998047]  [A loss: 0.074569, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "14: [D loss: 0.032796, acc: 0.998047]  [A loss: 0.068126, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "15: [D loss: 0.023899, acc: 1.000000]  [A loss: 0.151116, acc: 0.957031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "16: [D loss: 0.024722, acc: 0.998047]  [A loss: 0.054261, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "17: [D loss: 0.021323, acc: 0.998047]  [A loss: 0.055562, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "18: [D loss: 0.015737, acc: 1.000000]  [A loss: 0.109464, acc: 0.980469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "19: [D loss: 0.014632, acc: 1.000000]  [A loss: 0.060217, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "20: [D loss: 0.012121, acc: 1.000000]  [A loss: 0.130506, acc: 0.964844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "21: [D loss: 0.010916, acc: 1.000000]  [A loss: 0.120075, acc: 0.972656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "22: [D loss: 0.009554, acc: 1.000000]  [A loss: 0.256558, acc: 0.894531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "23: [D loss: 0.010011, acc: 0.998047]  [A loss: 0.044814, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "24: [D loss: 0.009827, acc: 0.998047]  [A loss: 0.102474, acc: 0.980469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "25: [D loss: 0.007214, acc: 1.000000]  [A loss: 0.149949, acc: 0.972656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "26: [D loss: 0.005015, acc: 1.000000]  [A loss: 0.190987, acc: 0.941406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "27: [D loss: 0.005105, acc: 1.000000]  [A loss: 0.273583, acc: 0.890625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "28: [D loss: 0.005349, acc: 1.000000]  [A loss: 0.053442, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "29: [D loss: 0.004757, acc: 1.000000]  [A loss: 0.416159, acc: 0.832031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "30: [D loss: 0.004548, acc: 1.000000]  [A loss: 0.059124, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "31: [D loss: 0.003997, acc: 1.000000]  [A loss: 0.710536, acc: 0.667969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "32: [D loss: 0.007275, acc: 0.998047]  [A loss: 0.000386, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "33: [D loss: 0.010699, acc: 1.000000]  [A loss: 4.514259, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "34: [D loss: 0.023878, acc: 0.992188]  [A loss: 0.000000, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "35: [D loss: 0.874409, acc: 0.505859]  [A loss: 18.397535, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "36: [D loss: 1.358452, acc: 0.611328]  [A loss: 0.000004, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "37: [D loss: 0.054237, acc: 1.000000]  [A loss: 0.000102, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "38: [D loss: 0.031945, acc: 1.000000]  [A loss: 0.000058, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "39: [D loss: 0.025291, acc: 1.000000]  [A loss: 0.000245, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "40: [D loss: 0.021589, acc: 1.000000]  [A loss: 0.000638, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "41: [D loss: 0.020724, acc: 1.000000]  [A loss: 0.001731, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "42: [D loss: 0.018345, acc: 1.000000]  [A loss: 0.003743, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "43: [D loss: 0.020776, acc: 1.000000]  [A loss: 0.017834, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "44: [D loss: 0.018452, acc: 1.000000]  [A loss: 0.048777, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "45: [D loss: 0.029813, acc: 0.998047]  [A loss: 0.457770, acc: 0.812500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "46: [D loss: 0.041301, acc: 0.996094]  [A loss: 3.903889, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "47: [D loss: 0.013053, acc: 0.998047]  [A loss: 3.785573, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "48: [D loss: 0.023125, acc: 0.998047]  [A loss: 4.078678, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "49: [D loss: 0.019141, acc: 1.000000]  [A loss: 5.027037, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "50: [D loss: 0.020366, acc: 0.998047]  [A loss: 4.470135, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "51: [D loss: 0.020458, acc: 1.000000]  [A loss: 5.967163, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "52: [D loss: 0.027494, acc: 0.996094]  [A loss: 4.198919, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "53: [D loss: 0.025543, acc: 1.000000]  [A loss: 8.083347, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "54: [D loss: 0.043232, acc: 0.990234]  [A loss: 2.119535, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "55: [D loss: 0.112698, acc: 0.966797]  [A loss: 22.972033, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "56: [D loss: 2.351539, acc: 0.537109]  [A loss: 0.009391, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "57: [D loss: 1.733703, acc: 0.500000]  [A loss: 10.790481, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "58: [D loss: 0.217330, acc: 0.916016]  [A loss: 6.935762, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "59: [D loss: 0.086717, acc: 0.984375]  [A loss: 5.740445, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "60: [D loss: 0.084994, acc: 0.996094]  [A loss: 5.335722, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "61: [D loss: 0.106843, acc: 0.988281]  [A loss: 4.651444, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "62: [D loss: 0.101086, acc: 0.992188]  [A loss: 4.683079, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "63: [D loss: 0.104916, acc: 0.988281]  [A loss: 4.420173, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "64: [D loss: 0.096297, acc: 0.990234]  [A loss: 4.373605, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "65: [D loss: 0.125172, acc: 0.976562]  [A loss: 3.733853, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "66: [D loss: 0.144019, acc: 0.972656]  [A loss: 4.256460, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "67: [D loss: 0.131218, acc: 0.984375]  [A loss: 3.139130, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "68: [D loss: 0.147025, acc: 0.960938]  [A loss: 4.417684, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "69: [D loss: 0.168563, acc: 0.957031]  [A loss: 1.957641, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "70: [D loss: 0.200415, acc: 0.919922]  [A loss: 6.869687, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "71: [D loss: 0.397555, acc: 0.845703]  [A loss: 0.284916, acc: 0.910156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "72: [D loss: 0.858466, acc: 0.576172]  [A loss: 7.691848, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "73: [D loss: 0.811802, acc: 0.701172]  [A loss: 1.055556, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "74: [D loss: 0.252093, acc: 0.882812]  [A loss: 3.541474, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "75: [D loss: 0.102182, acc: 0.968750]  [A loss: 2.200934, acc: 0.031250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "76: [D loss: 0.080030, acc: 0.986328]  [A loss: 1.910707, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "77: [D loss: 0.069856, acc: 0.990234]  [A loss: 1.854808, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "78: [D loss: 0.065602, acc: 0.990234]  [A loss: 2.014998, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "79: [D loss: 0.072846, acc: 0.994141]  [A loss: 2.060192, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "80: [D loss: 0.089647, acc: 0.982422]  [A loss: 1.653836, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "81: [D loss: 0.087107, acc: 0.984375]  [A loss: 1.453983, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "82: [D loss: 0.075380, acc: 0.988281]  [A loss: 2.320637, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "83: [D loss: 0.130368, acc: 0.957031]  [A loss: 0.938005, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "84: [D loss: 0.118468, acc: 0.972656]  [A loss: 3.242039, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "85: [D loss: 0.216884, acc: 0.923828]  [A loss: 0.268936, acc: 0.917969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "86: [D loss: 0.644582, acc: 0.642578]  [A loss: 8.330889, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "87: [D loss: 2.015086, acc: 0.552734]  [A loss: 0.909707, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "88: [D loss: 0.316997, acc: 0.833984]  [A loss: 2.228798, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "89: [D loss: 0.200099, acc: 0.951172]  [A loss: 1.221500, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "90: [D loss: 0.149667, acc: 0.980469]  [A loss: 1.378251, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "91: [D loss: 0.135671, acc: 0.972656]  [A loss: 1.350142, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "92: [D loss: 0.141343, acc: 0.968750]  [A loss: 1.244899, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "93: [D loss: 0.177669, acc: 0.966797]  [A loss: 1.582371, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "94: [D loss: 0.171303, acc: 0.957031]  [A loss: 1.329828, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "95: [D loss: 0.175031, acc: 0.962891]  [A loss: 1.454291, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "96: [D loss: 0.249173, acc: 0.945312]  [A loss: 1.041044, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "97: [D loss: 0.204185, acc: 0.951172]  [A loss: 2.273267, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "98: [D loss: 0.336840, acc: 0.900391]  [A loss: 0.628414, acc: 0.628906]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "99: [D loss: 0.425270, acc: 0.757812]  [A loss: 4.730140, acc: 0.000000]\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "100: [D loss: 1.076931, acc: 0.648438]  [A loss: 0.403506, acc: 0.871094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "101: [D loss: 0.796009, acc: 0.523438]  [A loss: 2.070335, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "102: [D loss: 0.396242, acc: 0.875000]  [A loss: 0.969626, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "103: [D loss: 0.323680, acc: 0.880859]  [A loss: 1.701093, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "104: [D loss: 0.282027, acc: 0.939453]  [A loss: 1.185074, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "105: [D loss: 0.271591, acc: 0.939453]  [A loss: 1.662350, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "106: [D loss: 0.318242, acc: 0.912109]  [A loss: 1.156270, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "107: [D loss: 0.303991, acc: 0.921875]  [A loss: 2.110604, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "108: [D loss: 0.390848, acc: 0.873047]  [A loss: 0.668179, acc: 0.585938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "109: [D loss: 0.461241, acc: 0.703125]  [A loss: 3.053204, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "110: [D loss: 0.663478, acc: 0.722656]  [A loss: 0.403282, acc: 0.917969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "111: [D loss: 0.733648, acc: 0.509766]  [A loss: 2.002621, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "112: [D loss: 0.401949, acc: 0.873047]  [A loss: 0.943311, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "113: [D loss: 0.366818, acc: 0.857422]  [A loss: 1.806568, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "114: [D loss: 0.370268, acc: 0.898438]  [A loss: 0.933699, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "115: [D loss: 0.354410, acc: 0.871094]  [A loss: 2.038531, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "116: [D loss: 0.388568, acc: 0.876953]  [A loss: 0.766027, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "117: [D loss: 0.434794, acc: 0.736328]  [A loss: 2.451818, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "118: [D loss: 0.499040, acc: 0.789062]  [A loss: 0.599294, acc: 0.652344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "119: [D loss: 0.577207, acc: 0.556641]  [A loss: 2.227089, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "120: [D loss: 0.508640, acc: 0.810547]  [A loss: 0.686504, acc: 0.570312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "121: [D loss: 0.517037, acc: 0.648438]  [A loss: 2.044308, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "122: [D loss: 0.451019, acc: 0.832031]  [A loss: 0.781106, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "123: [D loss: 0.456527, acc: 0.691406]  [A loss: 1.994863, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "124: [D loss: 0.468126, acc: 0.808594]  [A loss: 0.701087, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "125: [D loss: 0.485673, acc: 0.675781]  [A loss: 2.037519, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "126: [D loss: 0.455784, acc: 0.826172]  [A loss: 0.711509, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "127: [D loss: 0.507765, acc: 0.652344]  [A loss: 2.077929, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "128: [D loss: 0.492258, acc: 0.781250]  [A loss: 0.661589, acc: 0.582031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "129: [D loss: 0.531660, acc: 0.632812]  [A loss: 2.071077, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "130: [D loss: 0.513630, acc: 0.783203]  [A loss: 0.680283, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "131: [D loss: 0.488471, acc: 0.658203]  [A loss: 1.970065, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "132: [D loss: 0.479325, acc: 0.800781]  [A loss: 0.677749, acc: 0.578125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "133: [D loss: 0.487432, acc: 0.650391]  [A loss: 1.964141, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "134: [D loss: 0.500657, acc: 0.757812]  [A loss: 0.653819, acc: 0.613281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "135: [D loss: 0.503254, acc: 0.638672]  [A loss: 1.932815, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "136: [D loss: 0.483205, acc: 0.798828]  [A loss: 0.724887, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "137: [D loss: 0.502459, acc: 0.671875]  [A loss: 1.890580, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "138: [D loss: 0.520395, acc: 0.753906]  [A loss: 0.640591, acc: 0.613281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "139: [D loss: 0.520950, acc: 0.650391]  [A loss: 1.935043, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "140: [D loss: 0.518862, acc: 0.750000]  [A loss: 0.660853, acc: 0.597656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "141: [D loss: 0.529050, acc: 0.644531]  [A loss: 1.770001, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "142: [D loss: 0.485485, acc: 0.785156]  [A loss: 0.728129, acc: 0.507812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "143: [D loss: 0.493074, acc: 0.701172]  [A loss: 1.731835, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "144: [D loss: 0.477814, acc: 0.808594]  [A loss: 0.672781, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "145: [D loss: 0.500940, acc: 0.650391]  [A loss: 1.909158, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "146: [D loss: 0.589868, acc: 0.699219]  [A loss: 0.490679, acc: 0.832031]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "147: [D loss: 0.628436, acc: 0.558594]  [A loss: 1.734813, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "148: [D loss: 0.515637, acc: 0.726562]  [A loss: 0.748554, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "149: [D loss: 0.494547, acc: 0.687500]  [A loss: 1.639933, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "150: [D loss: 0.506932, acc: 0.791016]  [A loss: 0.686849, acc: 0.566406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "151: [D loss: 0.498202, acc: 0.693359]  [A loss: 1.661598, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "152: [D loss: 0.520524, acc: 0.759766]  [A loss: 0.612320, acc: 0.660156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "153: [D loss: 0.541690, acc: 0.640625]  [A loss: 1.824645, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "154: [D loss: 0.571125, acc: 0.683594]  [A loss: 0.646220, acc: 0.589844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "155: [D loss: 0.532373, acc: 0.607422]  [A loss: 1.556583, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "156: [D loss: 0.481741, acc: 0.798828]  [A loss: 0.766584, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "157: [D loss: 0.478435, acc: 0.726562]  [A loss: 1.682479, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "158: [D loss: 0.503880, acc: 0.787109]  [A loss: 0.680788, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "159: [D loss: 0.524926, acc: 0.644531]  [A loss: 1.998001, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "160: [D loss: 0.609583, acc: 0.638672]  [A loss: 0.592620, acc: 0.691406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "161: [D loss: 0.575533, acc: 0.558594]  [A loss: 1.576763, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "162: [D loss: 0.495448, acc: 0.771484]  [A loss: 0.730187, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "163: [D loss: 0.462880, acc: 0.736328]  [A loss: 1.526270, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "164: [D loss: 0.477805, acc: 0.820312]  [A loss: 0.722084, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "165: [D loss: 0.546033, acc: 0.625000]  [A loss: 2.010203, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "166: [D loss: 0.628555, acc: 0.578125]  [A loss: 0.688707, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "167: [D loss: 0.545613, acc: 0.601562]  [A loss: 1.385830, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "168: [D loss: 0.439024, acc: 0.875000]  [A loss: 0.874900, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "169: [D loss: 0.453517, acc: 0.748047]  [A loss: 1.613927, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "170: [D loss: 0.487705, acc: 0.783203]  [A loss: 0.733873, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "171: [D loss: 0.555071, acc: 0.611328]  [A loss: 1.914550, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "172: [D loss: 0.618780, acc: 0.597656]  [A loss: 0.688419, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "173: [D loss: 0.555430, acc: 0.597656]  [A loss: 1.475521, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "174: [D loss: 0.455299, acc: 0.830078]  [A loss: 0.838411, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "175: [D loss: 0.530186, acc: 0.705078]  [A loss: 1.718350, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "176: [D loss: 0.541624, acc: 0.689453]  [A loss: 0.681801, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "177: [D loss: 0.512222, acc: 0.644531]  [A loss: 1.507970, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "178: [D loss: 0.491272, acc: 0.771484]  [A loss: 0.714481, acc: 0.515625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "179: [D loss: 0.473969, acc: 0.730469]  [A loss: 1.534940, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "180: [D loss: 0.480777, acc: 0.787109]  [A loss: 0.649154, acc: 0.613281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "181: [D loss: 0.568009, acc: 0.603516]  [A loss: 1.877957, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "182: [D loss: 0.610328, acc: 0.576172]  [A loss: 0.644442, acc: 0.628906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "183: [D loss: 0.562231, acc: 0.583984]  [A loss: 1.537731, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "184: [D loss: 0.461757, acc: 0.787109]  [A loss: 0.756322, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "185: [D loss: 0.631868, acc: 0.597656]  [A loss: 1.932800, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "186: [D loss: 0.626183, acc: 0.591797]  [A loss: 0.764097, acc: 0.433594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "187: [D loss: 0.511518, acc: 0.705078]  [A loss: 1.363912, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "188: [D loss: 0.483359, acc: 0.789062]  [A loss: 0.902477, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "189: [D loss: 0.459811, acc: 0.794922]  [A loss: 1.478059, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "190: [D loss: 0.495049, acc: 0.777344]  [A loss: 0.722883, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "191: [D loss: 0.585455, acc: 0.626953]  [A loss: 1.860751, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "192: [D loss: 0.634192, acc: 0.587891]  [A loss: 0.645935, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "193: [D loss: 0.595119, acc: 0.589844]  [A loss: 1.554488, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "194: [D loss: 0.538424, acc: 0.705078]  [A loss: 0.740243, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "195: [D loss: 0.612560, acc: 0.619141]  [A loss: 1.562578, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "196: [D loss: 0.601480, acc: 0.628906]  [A loss: 0.736365, acc: 0.433594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "197: [D loss: 0.551541, acc: 0.669922]  [A loss: 1.399036, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "198: [D loss: 0.545495, acc: 0.724609]  [A loss: 0.769406, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "199: [D loss: 0.559998, acc: 0.662109]  [A loss: 1.496490, acc: 0.011719]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "200: [D loss: 0.582642, acc: 0.664062]  [A loss: 0.658371, acc: 0.582031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "201: [D loss: 0.578941, acc: 0.636719]  [A loss: 1.535009, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "202: [D loss: 0.570605, acc: 0.679688]  [A loss: 0.677069, acc: 0.574219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "203: [D loss: 0.564391, acc: 0.666016]  [A loss: 1.457682, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "204: [D loss: 0.575678, acc: 0.675781]  [A loss: 0.695570, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "205: [D loss: 0.563172, acc: 0.675781]  [A loss: 1.386062, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "206: [D loss: 0.546799, acc: 0.730469]  [A loss: 0.715312, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "207: [D loss: 0.561045, acc: 0.685547]  [A loss: 1.448172, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "208: [D loss: 0.603781, acc: 0.646484]  [A loss: 0.660727, acc: 0.628906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "209: [D loss: 0.588987, acc: 0.632812]  [A loss: 1.482072, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "210: [D loss: 0.636081, acc: 0.646484]  [A loss: 0.564031, acc: 0.710938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "211: [D loss: 0.651011, acc: 0.572266]  [A loss: 1.621311, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "212: [D loss: 0.654187, acc: 0.568359]  [A loss: 0.744180, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "213: [D loss: 0.606733, acc: 0.650391]  [A loss: 1.268847, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "214: [D loss: 0.541705, acc: 0.755859]  [A loss: 0.751728, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "215: [D loss: 0.598309, acc: 0.625000]  [A loss: 1.479800, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "216: [D loss: 0.599147, acc: 0.636719]  [A loss: 0.635761, acc: 0.667969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "217: [D loss: 0.589024, acc: 0.609375]  [A loss: 1.432182, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "218: [D loss: 0.593056, acc: 0.638672]  [A loss: 0.647318, acc: 0.613281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "219: [D loss: 0.605698, acc: 0.578125]  [A loss: 1.459473, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "220: [D loss: 0.587576, acc: 0.662109]  [A loss: 0.669182, acc: 0.585938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "221: [D loss: 0.561015, acc: 0.681641]  [A loss: 1.338573, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "222: [D loss: 0.565072, acc: 0.707031]  [A loss: 0.677616, acc: 0.585938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "223: [D loss: 0.553481, acc: 0.689453]  [A loss: 1.407328, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "224: [D loss: 0.593973, acc: 0.650391]  [A loss: 0.635909, acc: 0.628906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "225: [D loss: 0.585869, acc: 0.638672]  [A loss: 1.504554, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "226: [D loss: 0.619393, acc: 0.613281]  [A loss: 0.639448, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "227: [D loss: 0.582103, acc: 0.646484]  [A loss: 1.357413, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "228: [D loss: 0.597325, acc: 0.660156]  [A loss: 0.671458, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "229: [D loss: 0.592366, acc: 0.648438]  [A loss: 1.394697, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "230: [D loss: 0.618441, acc: 0.636719]  [A loss: 0.630542, acc: 0.648438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "231: [D loss: 0.580681, acc: 0.638672]  [A loss: 1.298703, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "232: [D loss: 0.601031, acc: 0.666016]  [A loss: 0.611780, acc: 0.699219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "233: [D loss: 0.590672, acc: 0.656250]  [A loss: 1.291386, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "234: [D loss: 0.628426, acc: 0.646484]  [A loss: 0.673753, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "235: [D loss: 0.575213, acc: 0.648438]  [A loss: 1.317419, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "236: [D loss: 0.595556, acc: 0.662109]  [A loss: 0.579151, acc: 0.703125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "237: [D loss: 0.607542, acc: 0.613281]  [A loss: 1.325489, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "238: [D loss: 0.618776, acc: 0.613281]  [A loss: 0.639521, acc: 0.613281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "239: [D loss: 0.633036, acc: 0.587891]  [A loss: 1.369351, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "240: [D loss: 0.614185, acc: 0.671875]  [A loss: 0.581216, acc: 0.664062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "241: [D loss: 0.632409, acc: 0.615234]  [A loss: 1.468593, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "242: [D loss: 0.629974, acc: 0.583984]  [A loss: 0.678243, acc: 0.550781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "243: [D loss: 0.581674, acc: 0.628906]  [A loss: 1.281424, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "244: [D loss: 0.585580, acc: 0.689453]  [A loss: 0.715624, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "245: [D loss: 0.595924, acc: 0.650391]  [A loss: 1.481617, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "246: [D loss: 0.639571, acc: 0.609375]  [A loss: 0.660332, acc: 0.589844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "247: [D loss: 0.604006, acc: 0.623047]  [A loss: 1.364963, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "248: [D loss: 0.604071, acc: 0.640625]  [A loss: 0.673397, acc: 0.609375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "249: [D loss: 0.603591, acc: 0.611328]  [A loss: 1.371028, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "250: [D loss: 0.626866, acc: 0.605469]  [A loss: 0.648853, acc: 0.632812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "251: [D loss: 0.600740, acc: 0.626953]  [A loss: 1.261458, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "252: [D loss: 0.619331, acc: 0.654297]  [A loss: 0.647127, acc: 0.601562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "253: [D loss: 0.631587, acc: 0.632812]  [A loss: 1.278459, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "254: [D loss: 0.631356, acc: 0.623047]  [A loss: 0.702948, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "255: [D loss: 0.617047, acc: 0.621094]  [A loss: 1.191064, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "256: [D loss: 0.609320, acc: 0.675781]  [A loss: 0.704393, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "257: [D loss: 0.600813, acc: 0.644531]  [A loss: 1.260544, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "258: [D loss: 0.619178, acc: 0.650391]  [A loss: 0.635561, acc: 0.632812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "259: [D loss: 0.618582, acc: 0.623047]  [A loss: 1.315246, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "260: [D loss: 0.627783, acc: 0.628906]  [A loss: 0.697937, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "261: [D loss: 0.639719, acc: 0.585938]  [A loss: 1.244666, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "262: [D loss: 0.629576, acc: 0.619141]  [A loss: 0.652217, acc: 0.605469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "263: [D loss: 0.624865, acc: 0.625000]  [A loss: 1.189708, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "264: [D loss: 0.614877, acc: 0.646484]  [A loss: 0.694764, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "265: [D loss: 0.617113, acc: 0.621094]  [A loss: 1.166983, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "266: [D loss: 0.620968, acc: 0.652344]  [A loss: 0.711931, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "267: [D loss: 0.632495, acc: 0.609375]  [A loss: 1.159947, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "268: [D loss: 0.641637, acc: 0.636719]  [A loss: 0.702425, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "269: [D loss: 0.610322, acc: 0.638672]  [A loss: 1.186455, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "270: [D loss: 0.640798, acc: 0.623047]  [A loss: 0.645193, acc: 0.601562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "271: [D loss: 0.640472, acc: 0.605469]  [A loss: 1.258274, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "272: [D loss: 0.656574, acc: 0.583984]  [A loss: 0.694418, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "273: [D loss: 0.652371, acc: 0.576172]  [A loss: 1.180453, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "274: [D loss: 0.625821, acc: 0.640625]  [A loss: 0.608211, acc: 0.699219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "275: [D loss: 0.635118, acc: 0.593750]  [A loss: 1.422935, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "276: [D loss: 0.683794, acc: 0.533203]  [A loss: 0.631494, acc: 0.632812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "277: [D loss: 0.630986, acc: 0.591797]  [A loss: 1.109656, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "278: [D loss: 0.631047, acc: 0.642578]  [A loss: 0.717778, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "279: [D loss: 0.611605, acc: 0.658203]  [A loss: 1.070678, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "280: [D loss: 0.615948, acc: 0.650391]  [A loss: 0.627572, acc: 0.656250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "281: [D loss: 0.613431, acc: 0.634766]  [A loss: 1.302028, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "282: [D loss: 0.651629, acc: 0.570312]  [A loss: 0.655520, acc: 0.597656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "283: [D loss: 0.630327, acc: 0.615234]  [A loss: 1.157922, acc: 0.031250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "284: [D loss: 0.629079, acc: 0.626953]  [A loss: 0.711338, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "285: [D loss: 0.616757, acc: 0.611328]  [A loss: 1.159864, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "286: [D loss: 0.623390, acc: 0.640625]  [A loss: 0.638048, acc: 0.628906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "287: [D loss: 0.622170, acc: 0.632812]  [A loss: 1.187693, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "288: [D loss: 0.610707, acc: 0.638672]  [A loss: 0.668252, acc: 0.574219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "289: [D loss: 0.611623, acc: 0.611328]  [A loss: 1.224089, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "290: [D loss: 0.634085, acc: 0.613281]  [A loss: 0.626239, acc: 0.644531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "291: [D loss: 0.609623, acc: 0.634766]  [A loss: 1.250850, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "292: [D loss: 0.631389, acc: 0.605469]  [A loss: 0.681009, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "293: [D loss: 0.620431, acc: 0.595703]  [A loss: 1.131744, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "294: [D loss: 0.634462, acc: 0.619141]  [A loss: 0.718189, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "295: [D loss: 0.628657, acc: 0.636719]  [A loss: 1.190990, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "296: [D loss: 0.626019, acc: 0.632812]  [A loss: 0.647084, acc: 0.625000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "297: [D loss: 0.635867, acc: 0.583984]  [A loss: 1.256038, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "298: [D loss: 0.630946, acc: 0.615234]  [A loss: 0.563062, acc: 0.738281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "299: [D loss: 0.640145, acc: 0.580078]  [A loss: 1.347414, acc: 0.007812]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "300: [D loss: 0.677449, acc: 0.535156]  [A loss: 0.702637, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "301: [D loss: 0.617062, acc: 0.638672]  [A loss: 0.954510, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "302: [D loss: 0.600631, acc: 0.681641]  [A loss: 0.731708, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "303: [D loss: 0.627328, acc: 0.650391]  [A loss: 1.084037, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "304: [D loss: 0.623241, acc: 0.613281]  [A loss: 0.629247, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "305: [D loss: 0.656579, acc: 0.580078]  [A loss: 1.216134, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "306: [D loss: 0.632782, acc: 0.621094]  [A loss: 0.607037, acc: 0.695312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "307: [D loss: 0.622822, acc: 0.613281]  [A loss: 1.224674, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "308: [D loss: 0.646289, acc: 0.576172]  [A loss: 0.699979, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "309: [D loss: 0.633850, acc: 0.625000]  [A loss: 1.042929, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "310: [D loss: 0.618438, acc: 0.652344]  [A loss: 0.725554, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "311: [D loss: 0.600834, acc: 0.685547]  [A loss: 1.039979, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "312: [D loss: 0.619732, acc: 0.658203]  [A loss: 0.699524, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "313: [D loss: 0.620205, acc: 0.642578]  [A loss: 1.174358, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "314: [D loss: 0.652460, acc: 0.597656]  [A loss: 0.625086, acc: 0.656250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "315: [D loss: 0.642154, acc: 0.619141]  [A loss: 1.282267, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "316: [D loss: 0.647867, acc: 0.587891]  [A loss: 0.607243, acc: 0.683594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "317: [D loss: 0.637115, acc: 0.613281]  [A loss: 1.210162, acc: 0.031250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "318: [D loss: 0.646418, acc: 0.574219]  [A loss: 0.711878, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "319: [D loss: 0.633917, acc: 0.630859]  [A loss: 1.039580, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "320: [D loss: 0.605311, acc: 0.673828]  [A loss: 0.706450, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "321: [D loss: 0.641062, acc: 0.630859]  [A loss: 1.149559, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "322: [D loss: 0.629390, acc: 0.619141]  [A loss: 0.690317, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "323: [D loss: 0.625709, acc: 0.632812]  [A loss: 1.115480, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "324: [D loss: 0.630746, acc: 0.646484]  [A loss: 0.589905, acc: 0.750000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "325: [D loss: 0.625108, acc: 0.607422]  [A loss: 1.298871, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "326: [D loss: 0.674298, acc: 0.566406]  [A loss: 0.677753, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "327: [D loss: 0.616832, acc: 0.625000]  [A loss: 1.029883, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "328: [D loss: 0.619829, acc: 0.640625]  [A loss: 0.707471, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "329: [D loss: 0.640173, acc: 0.630859]  [A loss: 1.070778, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "330: [D loss: 0.622671, acc: 0.615234]  [A loss: 0.655149, acc: 0.593750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "331: [D loss: 0.606624, acc: 0.660156]  [A loss: 1.080268, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "332: [D loss: 0.625460, acc: 0.656250]  [A loss: 0.648271, acc: 0.605469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "333: [D loss: 0.627937, acc: 0.617188]  [A loss: 1.209423, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "334: [D loss: 0.646967, acc: 0.623047]  [A loss: 0.599624, acc: 0.660156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "335: [D loss: 0.655163, acc: 0.578125]  [A loss: 1.196732, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "336: [D loss: 0.649521, acc: 0.585938]  [A loss: 0.719744, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "337: [D loss: 0.620897, acc: 0.673828]  [A loss: 0.898305, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "338: [D loss: 0.606625, acc: 0.685547]  [A loss: 0.790952, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "339: [D loss: 0.593162, acc: 0.724609]  [A loss: 0.918605, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "340: [D loss: 0.602981, acc: 0.689453]  [A loss: 0.787259, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "341: [D loss: 0.596173, acc: 0.701172]  [A loss: 0.978594, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "342: [D loss: 0.636125, acc: 0.642578]  [A loss: 0.754554, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "343: [D loss: 0.598556, acc: 0.677734]  [A loss: 1.189402, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "344: [D loss: 0.643711, acc: 0.623047]  [A loss: 0.410652, acc: 0.886719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "345: [D loss: 0.771544, acc: 0.521484]  [A loss: 1.581649, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "346: [D loss: 0.778634, acc: 0.513672]  [A loss: 0.785791, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "347: [D loss: 0.683284, acc: 0.552734]  [A loss: 0.983090, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "348: [D loss: 0.627963, acc: 0.642578]  [A loss: 0.815307, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "349: [D loss: 0.612961, acc: 0.683594]  [A loss: 0.892606, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "350: [D loss: 0.625923, acc: 0.648438]  [A loss: 0.855689, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "351: [D loss: 0.618943, acc: 0.662109]  [A loss: 0.829402, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "352: [D loss: 0.625969, acc: 0.652344]  [A loss: 0.921294, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "353: [D loss: 0.609780, acc: 0.671875]  [A loss: 0.833838, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "354: [D loss: 0.617098, acc: 0.673828]  [A loss: 0.893519, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "355: [D loss: 0.619864, acc: 0.666016]  [A loss: 0.794277, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "356: [D loss: 0.615242, acc: 0.656250]  [A loss: 0.974581, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "357: [D loss: 0.599463, acc: 0.699219]  [A loss: 0.772830, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "358: [D loss: 0.637822, acc: 0.640625]  [A loss: 1.174981, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "359: [D loss: 0.631491, acc: 0.615234]  [A loss: 0.516879, acc: 0.816406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "360: [D loss: 0.711394, acc: 0.548828]  [A loss: 1.590182, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "361: [D loss: 0.765864, acc: 0.509766]  [A loss: 0.702453, acc: 0.515625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "362: [D loss: 0.636608, acc: 0.646484]  [A loss: 0.913372, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "363: [D loss: 0.640745, acc: 0.648438]  [A loss: 0.780019, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "364: [D loss: 0.620535, acc: 0.654297]  [A loss: 0.894602, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "365: [D loss: 0.627815, acc: 0.666016]  [A loss: 0.800388, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "366: [D loss: 0.618320, acc: 0.691406]  [A loss: 0.923258, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "367: [D loss: 0.615691, acc: 0.689453]  [A loss: 0.781472, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "368: [D loss: 0.633262, acc: 0.654297]  [A loss: 1.038234, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "369: [D loss: 0.625747, acc: 0.634766]  [A loss: 0.689107, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "370: [D loss: 0.614036, acc: 0.675781]  [A loss: 1.217661, acc: 0.031250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "371: [D loss: 0.662004, acc: 0.576172]  [A loss: 0.561048, acc: 0.750000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "372: [D loss: 0.661117, acc: 0.580078]  [A loss: 1.252218, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "373: [D loss: 0.681028, acc: 0.558594]  [A loss: 0.710869, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "374: [D loss: 0.630488, acc: 0.615234]  [A loss: 0.967110, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "375: [D loss: 0.639735, acc: 0.646484]  [A loss: 0.714557, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "376: [D loss: 0.625612, acc: 0.654297]  [A loss: 1.019510, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "377: [D loss: 0.653613, acc: 0.628906]  [A loss: 0.710165, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "378: [D loss: 0.630870, acc: 0.656250]  [A loss: 1.042939, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "379: [D loss: 0.632334, acc: 0.626953]  [A loss: 0.714602, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "380: [D loss: 0.640884, acc: 0.644531]  [A loss: 1.098440, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "381: [D loss: 0.632856, acc: 0.646484]  [A loss: 0.696951, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "382: [D loss: 0.618707, acc: 0.662109]  [A loss: 1.136547, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "383: [D loss: 0.654655, acc: 0.613281]  [A loss: 0.699397, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "384: [D loss: 0.639317, acc: 0.628906]  [A loss: 1.107391, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "385: [D loss: 0.658032, acc: 0.585938]  [A loss: 0.654328, acc: 0.632812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "386: [D loss: 0.641555, acc: 0.611328]  [A loss: 1.107691, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "387: [D loss: 0.631319, acc: 0.644531]  [A loss: 0.632817, acc: 0.648438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "388: [D loss: 0.643174, acc: 0.605469]  [A loss: 1.156146, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "389: [D loss: 0.642476, acc: 0.628906]  [A loss: 0.686273, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "390: [D loss: 0.620868, acc: 0.630859]  [A loss: 0.988422, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "391: [D loss: 0.637847, acc: 0.646484]  [A loss: 0.734066, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "392: [D loss: 0.629306, acc: 0.664062]  [A loss: 1.075929, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "393: [D loss: 0.636132, acc: 0.642578]  [A loss: 0.661633, acc: 0.574219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "394: [D loss: 0.634650, acc: 0.611328]  [A loss: 1.126213, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "395: [D loss: 0.642058, acc: 0.611328]  [A loss: 0.664912, acc: 0.589844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "396: [D loss: 0.634439, acc: 0.613281]  [A loss: 1.116548, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "397: [D loss: 0.649154, acc: 0.628906]  [A loss: 0.684889, acc: 0.535156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "398: [D loss: 0.625507, acc: 0.644531]  [A loss: 1.112149, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "399: [D loss: 0.641281, acc: 0.634766]  [A loss: 0.700634, acc: 0.527344]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "400: [D loss: 0.658800, acc: 0.593750]  [A loss: 1.018887, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "401: [D loss: 0.653300, acc: 0.589844]  [A loss: 0.660965, acc: 0.613281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "402: [D loss: 0.644465, acc: 0.626953]  [A loss: 0.994807, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "403: [D loss: 0.640160, acc: 0.632812]  [A loss: 0.737313, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "404: [D loss: 0.629026, acc: 0.648438]  [A loss: 1.029683, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "405: [D loss: 0.654573, acc: 0.625000]  [A loss: 0.723459, acc: 0.500000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "406: [D loss: 0.618629, acc: 0.666016]  [A loss: 0.995129, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "407: [D loss: 0.654017, acc: 0.626953]  [A loss: 0.639108, acc: 0.625000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "408: [D loss: 0.644761, acc: 0.621094]  [A loss: 1.083858, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "409: [D loss: 0.644505, acc: 0.625000]  [A loss: 0.674795, acc: 0.546875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "410: [D loss: 0.658994, acc: 0.583984]  [A loss: 1.056288, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "411: [D loss: 0.630770, acc: 0.644531]  [A loss: 0.638491, acc: 0.632812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "412: [D loss: 0.650421, acc: 0.617188]  [A loss: 1.149787, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "413: [D loss: 0.645149, acc: 0.595703]  [A loss: 0.686440, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "414: [D loss: 0.633192, acc: 0.626953]  [A loss: 0.968053, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "415: [D loss: 0.621925, acc: 0.671875]  [A loss: 0.788973, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "416: [D loss: 0.619017, acc: 0.654297]  [A loss: 0.955846, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "417: [D loss: 0.634912, acc: 0.634766]  [A loss: 0.829310, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "418: [D loss: 0.626383, acc: 0.628906]  [A loss: 0.924094, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "419: [D loss: 0.632593, acc: 0.628906]  [A loss: 0.799864, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "420: [D loss: 0.621146, acc: 0.644531]  [A loss: 0.954415, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "421: [D loss: 0.632336, acc: 0.671875]  [A loss: 0.813015, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "422: [D loss: 0.609947, acc: 0.683594]  [A loss: 0.993475, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "423: [D loss: 0.654970, acc: 0.609375]  [A loss: 0.779438, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "424: [D loss: 0.610810, acc: 0.666016]  [A loss: 1.087800, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "425: [D loss: 0.642180, acc: 0.652344]  [A loss: 0.682465, acc: 0.550781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "426: [D loss: 0.642653, acc: 0.634766]  [A loss: 1.313124, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "427: [D loss: 0.704651, acc: 0.539062]  [A loss: 0.530065, acc: 0.812500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "428: [D loss: 0.676170, acc: 0.535156]  [A loss: 1.185915, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "429: [D loss: 0.669930, acc: 0.558594]  [A loss: 0.744912, acc: 0.433594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "430: [D loss: 0.644471, acc: 0.626953]  [A loss: 0.916288, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "431: [D loss: 0.623242, acc: 0.666016]  [A loss: 0.781134, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "432: [D loss: 0.645960, acc: 0.613281]  [A loss: 0.922533, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "433: [D loss: 0.635730, acc: 0.648438]  [A loss: 0.807453, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "434: [D loss: 0.618965, acc: 0.636719]  [A loss: 0.951592, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "435: [D loss: 0.638206, acc: 0.644531]  [A loss: 0.716473, acc: 0.515625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "436: [D loss: 0.607343, acc: 0.675781]  [A loss: 1.052288, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "437: [D loss: 0.631042, acc: 0.656250]  [A loss: 0.676307, acc: 0.578125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "438: [D loss: 0.647124, acc: 0.613281]  [A loss: 1.147101, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "439: [D loss: 0.649249, acc: 0.595703]  [A loss: 0.632277, acc: 0.636719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "440: [D loss: 0.645078, acc: 0.599609]  [A loss: 1.069969, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "441: [D loss: 0.644203, acc: 0.609375]  [A loss: 0.724495, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "442: [D loss: 0.646842, acc: 0.638672]  [A loss: 1.058149, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "443: [D loss: 0.646682, acc: 0.615234]  [A loss: 0.666412, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "444: [D loss: 0.645881, acc: 0.617188]  [A loss: 1.151457, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "445: [D loss: 0.684067, acc: 0.574219]  [A loss: 0.712874, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "446: [D loss: 0.630181, acc: 0.652344]  [A loss: 0.923990, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "447: [D loss: 0.612379, acc: 0.681641]  [A loss: 0.765620, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "448: [D loss: 0.632304, acc: 0.636719]  [A loss: 0.974464, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "449: [D loss: 0.640851, acc: 0.626953]  [A loss: 0.739812, acc: 0.507812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "450: [D loss: 0.644460, acc: 0.617188]  [A loss: 1.028036, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "451: [D loss: 0.643744, acc: 0.634766]  [A loss: 0.740502, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "452: [D loss: 0.665241, acc: 0.582031]  [A loss: 1.090639, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "453: [D loss: 0.652602, acc: 0.611328]  [A loss: 0.717221, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "454: [D loss: 0.627563, acc: 0.642578]  [A loss: 1.015558, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "455: [D loss: 0.647530, acc: 0.619141]  [A loss: 0.696576, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "456: [D loss: 0.632998, acc: 0.650391]  [A loss: 1.020146, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "457: [D loss: 0.638150, acc: 0.640625]  [A loss: 0.640541, acc: 0.609375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "458: [D loss: 0.626721, acc: 0.644531]  [A loss: 1.141340, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "459: [D loss: 0.658186, acc: 0.601562]  [A loss: 0.709559, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "460: [D loss: 0.643243, acc: 0.632812]  [A loss: 1.049275, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "461: [D loss: 0.637008, acc: 0.625000]  [A loss: 0.749392, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "462: [D loss: 0.630704, acc: 0.632812]  [A loss: 0.962726, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "463: [D loss: 0.621619, acc: 0.666016]  [A loss: 0.802839, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "464: [D loss: 0.632826, acc: 0.638672]  [A loss: 0.904455, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "465: [D loss: 0.646012, acc: 0.601562]  [A loss: 0.795086, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "466: [D loss: 0.623572, acc: 0.642578]  [A loss: 0.956885, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "467: [D loss: 0.616710, acc: 0.689453]  [A loss: 0.782326, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "468: [D loss: 0.611727, acc: 0.681641]  [A loss: 1.094655, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "469: [D loss: 0.647985, acc: 0.634766]  [A loss: 0.660098, acc: 0.601562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "470: [D loss: 0.654585, acc: 0.607422]  [A loss: 1.266310, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "471: [D loss: 0.674157, acc: 0.576172]  [A loss: 0.670193, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "472: [D loss: 0.636596, acc: 0.595703]  [A loss: 1.009231, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "473: [D loss: 0.633123, acc: 0.626953]  [A loss: 0.721643, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "474: [D loss: 0.635260, acc: 0.654297]  [A loss: 1.029555, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "475: [D loss: 0.633109, acc: 0.652344]  [A loss: 0.752563, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "476: [D loss: 0.644623, acc: 0.640625]  [A loss: 0.962015, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "477: [D loss: 0.615169, acc: 0.693359]  [A loss: 0.750897, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "478: [D loss: 0.652294, acc: 0.626953]  [A loss: 1.125090, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "479: [D loss: 0.641232, acc: 0.617188]  [A loss: 0.667773, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "480: [D loss: 0.650542, acc: 0.582031]  [A loss: 1.081625, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "481: [D loss: 0.637475, acc: 0.626953]  [A loss: 0.701702, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "482: [D loss: 0.631501, acc: 0.646484]  [A loss: 1.114219, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "483: [D loss: 0.652897, acc: 0.597656]  [A loss: 0.721823, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "484: [D loss: 0.634205, acc: 0.613281]  [A loss: 1.007419, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "485: [D loss: 0.630579, acc: 0.638672]  [A loss: 0.748127, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "486: [D loss: 0.614875, acc: 0.687500]  [A loss: 1.004462, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "487: [D loss: 0.622080, acc: 0.638672]  [A loss: 0.827791, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "488: [D loss: 0.650504, acc: 0.617188]  [A loss: 1.039843, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "489: [D loss: 0.650441, acc: 0.626953]  [A loss: 0.760800, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "490: [D loss: 0.631081, acc: 0.656250]  [A loss: 0.961258, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "491: [D loss: 0.627368, acc: 0.654297]  [A loss: 0.785213, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "492: [D loss: 0.625013, acc: 0.658203]  [A loss: 1.074093, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "493: [D loss: 0.646561, acc: 0.611328]  [A loss: 0.758987, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "494: [D loss: 0.646777, acc: 0.652344]  [A loss: 1.028808, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "495: [D loss: 0.650907, acc: 0.607422]  [A loss: 0.691008, acc: 0.546875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "496: [D loss: 0.637561, acc: 0.595703]  [A loss: 1.115753, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "497: [D loss: 0.651272, acc: 0.589844]  [A loss: 0.657924, acc: 0.550781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "498: [D loss: 0.622675, acc: 0.603516]  [A loss: 1.085983, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "499: [D loss: 0.640770, acc: 0.638672]  [A loss: 0.698027, acc: 0.496094]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "500: [D loss: 0.639732, acc: 0.597656]  [A loss: 1.067945, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "501: [D loss: 0.648302, acc: 0.625000]  [A loss: 0.722284, acc: 0.460938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "502: [D loss: 0.641134, acc: 0.623047]  [A loss: 1.001270, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "503: [D loss: 0.642781, acc: 0.648438]  [A loss: 0.738244, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "504: [D loss: 0.632740, acc: 0.650391]  [A loss: 1.022471, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "505: [D loss: 0.639828, acc: 0.632812]  [A loss: 0.741299, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "506: [D loss: 0.643835, acc: 0.599609]  [A loss: 1.035945, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "507: [D loss: 0.624901, acc: 0.636719]  [A loss: 0.749525, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "508: [D loss: 0.641613, acc: 0.638672]  [A loss: 1.093306, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "509: [D loss: 0.654073, acc: 0.609375]  [A loss: 0.780820, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "510: [D loss: 0.635157, acc: 0.642578]  [A loss: 0.984108, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "511: [D loss: 0.626635, acc: 0.658203]  [A loss: 0.799924, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "512: [D loss: 0.630458, acc: 0.656250]  [A loss: 0.981225, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "513: [D loss: 0.628734, acc: 0.648438]  [A loss: 0.751787, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "514: [D loss: 0.629218, acc: 0.628906]  [A loss: 1.178347, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "515: [D loss: 0.650830, acc: 0.619141]  [A loss: 0.659479, acc: 0.593750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "516: [D loss: 0.650507, acc: 0.605469]  [A loss: 1.152528, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "517: [D loss: 0.662679, acc: 0.585938]  [A loss: 0.785620, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "518: [D loss: 0.646909, acc: 0.587891]  [A loss: 0.966042, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "519: [D loss: 0.626484, acc: 0.640625]  [A loss: 0.793325, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "520: [D loss: 0.646930, acc: 0.632812]  [A loss: 0.991740, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "521: [D loss: 0.636414, acc: 0.654297]  [A loss: 0.802003, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "522: [D loss: 0.632608, acc: 0.644531]  [A loss: 0.997292, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "523: [D loss: 0.625201, acc: 0.621094]  [A loss: 0.819810, acc: 0.343750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "524: [D loss: 0.637912, acc: 0.650391]  [A loss: 0.935916, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "525: [D loss: 0.626912, acc: 0.658203]  [A loss: 0.790678, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "526: [D loss: 0.631529, acc: 0.636719]  [A loss: 1.061095, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "527: [D loss: 0.636863, acc: 0.619141]  [A loss: 0.716971, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "528: [D loss: 0.637553, acc: 0.623047]  [A loss: 1.155290, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "529: [D loss: 0.656536, acc: 0.587891]  [A loss: 0.660174, acc: 0.582031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "530: [D loss: 0.631877, acc: 0.630859]  [A loss: 1.180910, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "531: [D loss: 0.638438, acc: 0.615234]  [A loss: 0.728669, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "532: [D loss: 0.669012, acc: 0.603516]  [A loss: 1.009416, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "533: [D loss: 0.625949, acc: 0.660156]  [A loss: 0.763708, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "534: [D loss: 0.619372, acc: 0.646484]  [A loss: 0.939443, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "535: [D loss: 0.627161, acc: 0.652344]  [A loss: 0.847036, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "536: [D loss: 0.621635, acc: 0.656250]  [A loss: 0.882746, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "537: [D loss: 0.621614, acc: 0.650391]  [A loss: 0.907016, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "538: [D loss: 0.633620, acc: 0.640625]  [A loss: 0.827449, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "539: [D loss: 0.663508, acc: 0.607422]  [A loss: 1.048796, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "540: [D loss: 0.649200, acc: 0.636719]  [A loss: 0.689942, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "541: [D loss: 0.637950, acc: 0.617188]  [A loss: 1.101437, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "542: [D loss: 0.657753, acc: 0.615234]  [A loss: 0.708438, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "543: [D loss: 0.644392, acc: 0.626953]  [A loss: 1.064913, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "544: [D loss: 0.654156, acc: 0.623047]  [A loss: 0.745778, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "545: [D loss: 0.650779, acc: 0.599609]  [A loss: 0.997387, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "546: [D loss: 0.640451, acc: 0.654297]  [A loss: 0.745202, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "547: [D loss: 0.623154, acc: 0.650391]  [A loss: 0.973529, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "548: [D loss: 0.626131, acc: 0.650391]  [A loss: 0.822637, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "549: [D loss: 0.621589, acc: 0.656250]  [A loss: 0.976253, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "550: [D loss: 0.631527, acc: 0.646484]  [A loss: 0.802131, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "551: [D loss: 0.619415, acc: 0.689453]  [A loss: 0.878945, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "552: [D loss: 0.630018, acc: 0.648438]  [A loss: 0.898740, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "553: [D loss: 0.624530, acc: 0.638672]  [A loss: 1.045336, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "554: [D loss: 0.619106, acc: 0.683594]  [A loss: 0.733282, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "555: [D loss: 0.631876, acc: 0.623047]  [A loss: 1.185016, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "556: [D loss: 0.641016, acc: 0.617188]  [A loss: 0.617415, acc: 0.664062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "557: [D loss: 0.671808, acc: 0.607422]  [A loss: 1.192141, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "558: [D loss: 0.672651, acc: 0.595703]  [A loss: 0.694842, acc: 0.500000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "559: [D loss: 0.623044, acc: 0.642578]  [A loss: 1.007063, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "560: [D loss: 0.651490, acc: 0.597656]  [A loss: 0.835062, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "561: [D loss: 0.617163, acc: 0.650391]  [A loss: 0.929591, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "562: [D loss: 0.617263, acc: 0.687500]  [A loss: 0.846272, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "563: [D loss: 0.620499, acc: 0.677734]  [A loss: 0.913169, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "564: [D loss: 0.603713, acc: 0.671875]  [A loss: 0.817207, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "565: [D loss: 0.615807, acc: 0.675781]  [A loss: 1.067403, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "566: [D loss: 0.654609, acc: 0.609375]  [A loss: 0.699007, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "567: [D loss: 0.626740, acc: 0.615234]  [A loss: 1.156171, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "568: [D loss: 0.644690, acc: 0.617188]  [A loss: 0.758310, acc: 0.421875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "569: [D loss: 0.655029, acc: 0.625000]  [A loss: 1.072375, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "570: [D loss: 0.608928, acc: 0.683594]  [A loss: 0.747460, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "571: [D loss: 0.630365, acc: 0.626953]  [A loss: 1.064036, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "572: [D loss: 0.614122, acc: 0.658203]  [A loss: 0.740213, acc: 0.460938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "573: [D loss: 0.639602, acc: 0.601562]  [A loss: 1.093149, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "574: [D loss: 0.612865, acc: 0.681641]  [A loss: 0.788191, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "575: [D loss: 0.627782, acc: 0.619141]  [A loss: 1.004352, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "576: [D loss: 0.618898, acc: 0.679688]  [A loss: 0.866907, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "577: [D loss: 0.626314, acc: 0.632812]  [A loss: 1.007570, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "578: [D loss: 0.625002, acc: 0.652344]  [A loss: 0.868569, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "579: [D loss: 0.619651, acc: 0.660156]  [A loss: 0.928385, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "580: [D loss: 0.606622, acc: 0.666016]  [A loss: 0.876213, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "581: [D loss: 0.609218, acc: 0.667969]  [A loss: 1.049226, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "582: [D loss: 0.648739, acc: 0.632812]  [A loss: 0.735947, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "583: [D loss: 0.637486, acc: 0.623047]  [A loss: 1.164627, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "584: [D loss: 0.646986, acc: 0.630859]  [A loss: 0.648277, acc: 0.593750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "585: [D loss: 0.655626, acc: 0.619141]  [A loss: 1.177143, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "586: [D loss: 0.660257, acc: 0.605469]  [A loss: 0.723341, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "587: [D loss: 0.615337, acc: 0.646484]  [A loss: 0.983199, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "588: [D loss: 0.620186, acc: 0.654297]  [A loss: 0.949138, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "589: [D loss: 0.635849, acc: 0.658203]  [A loss: 0.876213, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "590: [D loss: 0.620359, acc: 0.664062]  [A loss: 0.943785, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "591: [D loss: 0.634782, acc: 0.623047]  [A loss: 0.884199, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "592: [D loss: 0.604680, acc: 0.677734]  [A loss: 0.961377, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "593: [D loss: 0.601531, acc: 0.693359]  [A loss: 0.881191, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "594: [D loss: 0.608070, acc: 0.703125]  [A loss: 1.011085, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "595: [D loss: 0.658752, acc: 0.597656]  [A loss: 0.952744, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "596: [D loss: 0.610423, acc: 0.691406]  [A loss: 0.882397, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "597: [D loss: 0.645042, acc: 0.638672]  [A loss: 0.943311, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "598: [D loss: 0.645728, acc: 0.636719]  [A loss: 0.973496, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "599: [D loss: 0.636522, acc: 0.650391]  [A loss: 0.781133, acc: 0.406250]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "600: [D loss: 0.628975, acc: 0.646484]  [A loss: 1.100306, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "601: [D loss: 0.636701, acc: 0.630859]  [A loss: 0.721955, acc: 0.507812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "602: [D loss: 0.644020, acc: 0.613281]  [A loss: 1.286815, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "603: [D loss: 0.656662, acc: 0.597656]  [A loss: 0.724002, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "604: [D loss: 0.628651, acc: 0.654297]  [A loss: 1.065839, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "605: [D loss: 0.633924, acc: 0.636719]  [A loss: 0.795404, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "606: [D loss: 0.610241, acc: 0.664062]  [A loss: 0.979481, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "607: [D loss: 0.635665, acc: 0.644531]  [A loss: 0.867963, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "608: [D loss: 0.612431, acc: 0.681641]  [A loss: 0.982566, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "609: [D loss: 0.620509, acc: 0.666016]  [A loss: 0.846036, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "610: [D loss: 0.637075, acc: 0.648438]  [A loss: 0.958251, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "611: [D loss: 0.620202, acc: 0.644531]  [A loss: 0.788366, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "612: [D loss: 0.618971, acc: 0.666016]  [A loss: 1.096445, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "613: [D loss: 0.599802, acc: 0.697266]  [A loss: 0.660839, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "614: [D loss: 0.649475, acc: 0.583984]  [A loss: 1.246672, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "615: [D loss: 0.651775, acc: 0.626953]  [A loss: 0.689849, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "616: [D loss: 0.610517, acc: 0.642578]  [A loss: 1.108761, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "617: [D loss: 0.648637, acc: 0.623047]  [A loss: 0.800077, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "618: [D loss: 0.624132, acc: 0.628906]  [A loss: 0.974804, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "619: [D loss: 0.633330, acc: 0.625000]  [A loss: 0.831672, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "620: [D loss: 0.617157, acc: 0.667969]  [A loss: 1.034260, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "621: [D loss: 0.618542, acc: 0.650391]  [A loss: 0.780628, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "622: [D loss: 0.634588, acc: 0.638672]  [A loss: 1.075792, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "623: [D loss: 0.602854, acc: 0.660156]  [A loss: 0.851062, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "624: [D loss: 0.608611, acc: 0.679688]  [A loss: 1.096402, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "625: [D loss: 0.615393, acc: 0.652344]  [A loss: 0.748294, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "626: [D loss: 0.648391, acc: 0.640625]  [A loss: 1.135852, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "627: [D loss: 0.640222, acc: 0.632812]  [A loss: 0.746641, acc: 0.460938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "628: [D loss: 0.635688, acc: 0.609375]  [A loss: 1.072218, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "629: [D loss: 0.627321, acc: 0.634766]  [A loss: 0.766993, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "630: [D loss: 0.600187, acc: 0.667969]  [A loss: 1.048936, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "631: [D loss: 0.645195, acc: 0.628906]  [A loss: 0.799121, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "632: [D loss: 0.620852, acc: 0.656250]  [A loss: 1.024173, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "633: [D loss: 0.618649, acc: 0.669922]  [A loss: 0.919637, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "634: [D loss: 0.629555, acc: 0.632812]  [A loss: 0.981966, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "635: [D loss: 0.636577, acc: 0.623047]  [A loss: 0.951248, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "636: [D loss: 0.609905, acc: 0.666016]  [A loss: 0.843655, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "637: [D loss: 0.643328, acc: 0.621094]  [A loss: 1.061111, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "638: [D loss: 0.614309, acc: 0.681641]  [A loss: 0.790584, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "639: [D loss: 0.615068, acc: 0.650391]  [A loss: 1.118649, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "640: [D loss: 0.616636, acc: 0.667969]  [A loss: 0.697224, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "641: [D loss: 0.667715, acc: 0.593750]  [A loss: 1.262792, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "642: [D loss: 0.655979, acc: 0.625000]  [A loss: 0.811860, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "643: [D loss: 0.653291, acc: 0.591797]  [A loss: 1.078282, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "644: [D loss: 0.611937, acc: 0.666016]  [A loss: 0.844310, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "645: [D loss: 0.630482, acc: 0.650391]  [A loss: 1.087582, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "646: [D loss: 0.616875, acc: 0.640625]  [A loss: 0.816898, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "647: [D loss: 0.615122, acc: 0.646484]  [A loss: 0.985740, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "648: [D loss: 0.618205, acc: 0.675781]  [A loss: 0.825208, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "649: [D loss: 0.635289, acc: 0.662109]  [A loss: 1.061256, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "650: [D loss: 0.639056, acc: 0.623047]  [A loss: 0.794207, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "651: [D loss: 0.645272, acc: 0.621094]  [A loss: 1.054145, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "652: [D loss: 0.641250, acc: 0.636719]  [A loss: 0.775063, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "653: [D loss: 0.627415, acc: 0.636719]  [A loss: 1.075585, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "654: [D loss: 0.623682, acc: 0.652344]  [A loss: 0.829153, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "655: [D loss: 0.608963, acc: 0.673828]  [A loss: 1.023968, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "656: [D loss: 0.602633, acc: 0.677734]  [A loss: 0.795758, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "657: [D loss: 0.630697, acc: 0.603516]  [A loss: 1.093954, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "658: [D loss: 0.639132, acc: 0.640625]  [A loss: 0.800143, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "659: [D loss: 0.609187, acc: 0.666016]  [A loss: 1.075821, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "660: [D loss: 0.618759, acc: 0.648438]  [A loss: 0.812411, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "661: [D loss: 0.627431, acc: 0.650391]  [A loss: 1.117511, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "662: [D loss: 0.636914, acc: 0.640625]  [A loss: 0.731283, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "663: [D loss: 0.641773, acc: 0.623047]  [A loss: 1.157595, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "664: [D loss: 0.647963, acc: 0.628906]  [A loss: 0.769203, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "665: [D loss: 0.642299, acc: 0.658203]  [A loss: 1.065868, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "666: [D loss: 0.613770, acc: 0.642578]  [A loss: 0.788539, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "667: [D loss: 0.633017, acc: 0.656250]  [A loss: 0.981471, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "668: [D loss: 0.651666, acc: 0.621094]  [A loss: 0.817806, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "669: [D loss: 0.619757, acc: 0.652344]  [A loss: 0.933581, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "670: [D loss: 0.627492, acc: 0.662109]  [A loss: 0.967648, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "671: [D loss: 0.640736, acc: 0.623047]  [A loss: 0.827941, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "672: [D loss: 0.600845, acc: 0.681641]  [A loss: 1.063581, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "673: [D loss: 0.627213, acc: 0.644531]  [A loss: 0.855518, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "674: [D loss: 0.631917, acc: 0.623047]  [A loss: 1.053945, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "675: [D loss: 0.626090, acc: 0.660156]  [A loss: 0.776635, acc: 0.433594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "676: [D loss: 0.629856, acc: 0.644531]  [A loss: 1.176019, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "677: [D loss: 0.635372, acc: 0.656250]  [A loss: 0.735584, acc: 0.472656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "678: [D loss: 0.650311, acc: 0.625000]  [A loss: 1.078915, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "679: [D loss: 0.625469, acc: 0.658203]  [A loss: 0.819875, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "680: [D loss: 0.625277, acc: 0.634766]  [A loss: 0.995809, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "681: [D loss: 0.674011, acc: 0.591797]  [A loss: 0.958808, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "682: [D loss: 0.612875, acc: 0.685547]  [A loss: 0.902665, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "683: [D loss: 0.611903, acc: 0.685547]  [A loss: 0.950542, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "684: [D loss: 0.616736, acc: 0.650391]  [A loss: 0.979374, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "685: [D loss: 0.630993, acc: 0.652344]  [A loss: 0.915555, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "686: [D loss: 0.603036, acc: 0.691406]  [A loss: 0.952949, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "687: [D loss: 0.625380, acc: 0.628906]  [A loss: 1.089313, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "688: [D loss: 0.606191, acc: 0.681641]  [A loss: 0.762639, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "689: [D loss: 0.615509, acc: 0.656250]  [A loss: 1.308337, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "690: [D loss: 0.665228, acc: 0.613281]  [A loss: 0.673765, acc: 0.589844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "691: [D loss: 0.636017, acc: 0.611328]  [A loss: 1.216181, acc: 0.062500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "692: [D loss: 0.641348, acc: 0.601562]  [A loss: 0.790222, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "693: [D loss: 0.637858, acc: 0.638672]  [A loss: 1.058164, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "694: [D loss: 0.601887, acc: 0.675781]  [A loss: 0.802201, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "695: [D loss: 0.615681, acc: 0.683594]  [A loss: 0.988607, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "696: [D loss: 0.609612, acc: 0.685547]  [A loss: 0.910750, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "697: [D loss: 0.598066, acc: 0.693359]  [A loss: 0.960118, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "698: [D loss: 0.610150, acc: 0.671875]  [A loss: 0.912094, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "699: [D loss: 0.594707, acc: 0.673828]  [A loss: 1.054641, acc: 0.175781]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "700: [D loss: 0.593847, acc: 0.689453]  [A loss: 0.832724, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "701: [D loss: 0.630324, acc: 0.621094]  [A loss: 1.184228, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "702: [D loss: 0.641805, acc: 0.640625]  [A loss: 0.668136, acc: 0.546875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "703: [D loss: 0.659125, acc: 0.607422]  [A loss: 1.179630, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "704: [D loss: 0.658246, acc: 0.599609]  [A loss: 0.740474, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "705: [D loss: 0.635007, acc: 0.650391]  [A loss: 1.029959, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "706: [D loss: 0.607237, acc: 0.683594]  [A loss: 0.821548, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "707: [D loss: 0.604289, acc: 0.675781]  [A loss: 1.076638, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "708: [D loss: 0.627957, acc: 0.617188]  [A loss: 0.842060, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "709: [D loss: 0.635703, acc: 0.630859]  [A loss: 0.917686, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "710: [D loss: 0.597470, acc: 0.697266]  [A loss: 0.879155, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "711: [D loss: 0.616567, acc: 0.666016]  [A loss: 0.972475, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "712: [D loss: 0.624799, acc: 0.671875]  [A loss: 0.959002, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "713: [D loss: 0.613428, acc: 0.689453]  [A loss: 0.808834, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "714: [D loss: 0.647762, acc: 0.646484]  [A loss: 1.242326, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "715: [D loss: 0.647354, acc: 0.611328]  [A loss: 0.700451, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "716: [D loss: 0.638836, acc: 0.619141]  [A loss: 1.102699, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "717: [D loss: 0.631135, acc: 0.650391]  [A loss: 0.789966, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "718: [D loss: 0.604760, acc: 0.658203]  [A loss: 1.122249, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "719: [D loss: 0.624538, acc: 0.654297]  [A loss: 0.794052, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "720: [D loss: 0.643667, acc: 0.640625]  [A loss: 1.046924, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "721: [D loss: 0.616517, acc: 0.656250]  [A loss: 0.924416, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "722: [D loss: 0.621607, acc: 0.652344]  [A loss: 0.944105, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "723: [D loss: 0.597592, acc: 0.679688]  [A loss: 0.953135, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "724: [D loss: 0.611823, acc: 0.677734]  [A loss: 0.973376, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "725: [D loss: 0.612153, acc: 0.644531]  [A loss: 0.963406, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "726: [D loss: 0.606099, acc: 0.701172]  [A loss: 1.006138, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "727: [D loss: 0.640192, acc: 0.634766]  [A loss: 0.876425, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "728: [D loss: 0.613876, acc: 0.671875]  [A loss: 1.133256, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "729: [D loss: 0.603704, acc: 0.646484]  [A loss: 0.814174, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "730: [D loss: 0.648645, acc: 0.580078]  [A loss: 1.222063, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "731: [D loss: 0.617870, acc: 0.644531]  [A loss: 0.738864, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "732: [D loss: 0.656073, acc: 0.636719]  [A loss: 1.178047, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "733: [D loss: 0.647609, acc: 0.636719]  [A loss: 0.838655, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "734: [D loss: 0.623673, acc: 0.648438]  [A loss: 1.008424, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "735: [D loss: 0.630661, acc: 0.626953]  [A loss: 0.907647, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "736: [D loss: 0.625224, acc: 0.640625]  [A loss: 0.921101, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "737: [D loss: 0.638576, acc: 0.646484]  [A loss: 0.978826, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "738: [D loss: 0.620298, acc: 0.671875]  [A loss: 0.863926, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "739: [D loss: 0.582510, acc: 0.716797]  [A loss: 0.974058, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "740: [D loss: 0.623584, acc: 0.679688]  [A loss: 0.903095, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "741: [D loss: 0.618511, acc: 0.671875]  [A loss: 1.070822, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "742: [D loss: 0.623076, acc: 0.658203]  [A loss: 0.831289, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "743: [D loss: 0.611775, acc: 0.660156]  [A loss: 1.198588, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "744: [D loss: 0.625511, acc: 0.642578]  [A loss: 0.759652, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "745: [D loss: 0.642423, acc: 0.625000]  [A loss: 1.164961, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "746: [D loss: 0.634614, acc: 0.626953]  [A loss: 0.774174, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "747: [D loss: 0.632742, acc: 0.648438]  [A loss: 1.084125, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "748: [D loss: 0.621952, acc: 0.673828]  [A loss: 0.834205, acc: 0.363281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "749: [D loss: 0.635356, acc: 0.650391]  [A loss: 1.077235, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "750: [D loss: 0.613407, acc: 0.681641]  [A loss: 0.899875, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "751: [D loss: 0.602540, acc: 0.681641]  [A loss: 1.101660, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "752: [D loss: 0.622933, acc: 0.652344]  [A loss: 0.868698, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "753: [D loss: 0.612687, acc: 0.669922]  [A loss: 1.069668, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "754: [D loss: 0.617620, acc: 0.656250]  [A loss: 0.885604, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "755: [D loss: 0.623541, acc: 0.667969]  [A loss: 0.999651, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "756: [D loss: 0.602294, acc: 0.654297]  [A loss: 0.947275, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "757: [D loss: 0.600666, acc: 0.689453]  [A loss: 0.982607, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "758: [D loss: 0.595699, acc: 0.691406]  [A loss: 0.870343, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "759: [D loss: 0.629741, acc: 0.650391]  [A loss: 1.051528, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "760: [D loss: 0.612701, acc: 0.656250]  [A loss: 0.824379, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "761: [D loss: 0.610013, acc: 0.662109]  [A loss: 1.199433, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "762: [D loss: 0.638256, acc: 0.625000]  [A loss: 0.741191, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "763: [D loss: 0.670510, acc: 0.611328]  [A loss: 1.267659, acc: 0.062500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "764: [D loss: 0.643869, acc: 0.634766]  [A loss: 0.788431, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "765: [D loss: 0.630959, acc: 0.642578]  [A loss: 1.047718, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "766: [D loss: 0.643693, acc: 0.628906]  [A loss: 0.840538, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "767: [D loss: 0.620120, acc: 0.644531]  [A loss: 1.043500, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "768: [D loss: 0.635836, acc: 0.617188]  [A loss: 0.897494, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "769: [D loss: 0.597877, acc: 0.664062]  [A loss: 0.896264, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "770: [D loss: 0.611909, acc: 0.669922]  [A loss: 0.973455, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "771: [D loss: 0.609615, acc: 0.660156]  [A loss: 0.908400, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "772: [D loss: 0.595033, acc: 0.687500]  [A loss: 0.948070, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "773: [D loss: 0.607238, acc: 0.677734]  [A loss: 0.881987, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "774: [D loss: 0.614536, acc: 0.671875]  [A loss: 1.090872, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "775: [D loss: 0.611092, acc: 0.693359]  [A loss: 0.945152, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "776: [D loss: 0.626186, acc: 0.636719]  [A loss: 1.147822, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "777: [D loss: 0.605204, acc: 0.650391]  [A loss: 0.758390, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "778: [D loss: 0.629214, acc: 0.642578]  [A loss: 1.426174, acc: 0.031250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "779: [D loss: 0.650594, acc: 0.617188]  [A loss: 0.681193, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "780: [D loss: 0.666845, acc: 0.587891]  [A loss: 1.158286, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "781: [D loss: 0.627302, acc: 0.636719]  [A loss: 0.816783, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "782: [D loss: 0.610741, acc: 0.673828]  [A loss: 0.966287, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "783: [D loss: 0.614893, acc: 0.664062]  [A loss: 0.947951, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "784: [D loss: 0.589630, acc: 0.687500]  [A loss: 0.836571, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "785: [D loss: 0.625290, acc: 0.662109]  [A loss: 1.074162, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "786: [D loss: 0.622204, acc: 0.673828]  [A loss: 0.936645, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "787: [D loss: 0.611495, acc: 0.699219]  [A loss: 1.022012, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "788: [D loss: 0.605933, acc: 0.664062]  [A loss: 0.905534, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "789: [D loss: 0.596909, acc: 0.708984]  [A loss: 1.031763, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "790: [D loss: 0.615043, acc: 0.644531]  [A loss: 0.851022, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "791: [D loss: 0.618018, acc: 0.664062]  [A loss: 1.093834, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "792: [D loss: 0.589898, acc: 0.693359]  [A loss: 0.910412, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "793: [D loss: 0.610904, acc: 0.660156]  [A loss: 1.018730, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "794: [D loss: 0.604546, acc: 0.679688]  [A loss: 0.954909, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "795: [D loss: 0.599978, acc: 0.658203]  [A loss: 1.091750, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "796: [D loss: 0.596460, acc: 0.691406]  [A loss: 0.818903, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "797: [D loss: 0.617756, acc: 0.634766]  [A loss: 1.262797, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "798: [D loss: 0.640001, acc: 0.644531]  [A loss: 0.763965, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "799: [D loss: 0.640022, acc: 0.638672]  [A loss: 1.298661, acc: 0.046875]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "800: [D loss: 0.644885, acc: 0.595703]  [A loss: 0.748748, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "801: [D loss: 0.657218, acc: 0.599609]  [A loss: 1.174246, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "802: [D loss: 0.618659, acc: 0.662109]  [A loss: 0.879950, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "803: [D loss: 0.626735, acc: 0.642578]  [A loss: 1.022189, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "804: [D loss: 0.621532, acc: 0.636719]  [A loss: 0.879761, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "805: [D loss: 0.633360, acc: 0.621094]  [A loss: 1.055972, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "806: [D loss: 0.591605, acc: 0.703125]  [A loss: 0.864462, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "807: [D loss: 0.611152, acc: 0.664062]  [A loss: 1.091680, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "808: [D loss: 0.622081, acc: 0.646484]  [A loss: 0.818245, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "809: [D loss: 0.598409, acc: 0.703125]  [A loss: 1.099643, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "810: [D loss: 0.611560, acc: 0.669922]  [A loss: 0.954670, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "811: [D loss: 0.628942, acc: 0.656250]  [A loss: 1.107047, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "812: [D loss: 0.610268, acc: 0.679688]  [A loss: 0.823602, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "813: [D loss: 0.631989, acc: 0.654297]  [A loss: 1.141384, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "814: [D loss: 0.647161, acc: 0.646484]  [A loss: 0.797473, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "815: [D loss: 0.618640, acc: 0.638672]  [A loss: 1.081298, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "816: [D loss: 0.600878, acc: 0.658203]  [A loss: 0.834675, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "817: [D loss: 0.633268, acc: 0.677734]  [A loss: 1.075308, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "818: [D loss: 0.594966, acc: 0.707031]  [A loss: 0.869856, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "819: [D loss: 0.601713, acc: 0.683594]  [A loss: 0.999430, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "820: [D loss: 0.593425, acc: 0.714844]  [A loss: 0.852288, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "821: [D loss: 0.598929, acc: 0.652344]  [A loss: 1.117997, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "822: [D loss: 0.606469, acc: 0.679688]  [A loss: 0.851815, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "823: [D loss: 0.609411, acc: 0.660156]  [A loss: 1.209559, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "824: [D loss: 0.604956, acc: 0.669922]  [A loss: 0.807526, acc: 0.433594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "825: [D loss: 0.671231, acc: 0.587891]  [A loss: 1.295767, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "826: [D loss: 0.621267, acc: 0.673828]  [A loss: 0.811548, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "827: [D loss: 0.618117, acc: 0.662109]  [A loss: 1.159893, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "828: [D loss: 0.595101, acc: 0.679688]  [A loss: 0.907319, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "829: [D loss: 0.613114, acc: 0.662109]  [A loss: 1.042063, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "830: [D loss: 0.612965, acc: 0.677734]  [A loss: 0.968611, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "831: [D loss: 0.624665, acc: 0.683594]  [A loss: 1.068931, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "832: [D loss: 0.606995, acc: 0.705078]  [A loss: 0.880535, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "833: [D loss: 0.625520, acc: 0.677734]  [A loss: 1.047346, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "834: [D loss: 0.610215, acc: 0.662109]  [A loss: 0.855291, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "835: [D loss: 0.592485, acc: 0.697266]  [A loss: 1.048233, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "836: [D loss: 0.589360, acc: 0.703125]  [A loss: 0.911165, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "837: [D loss: 0.561062, acc: 0.744141]  [A loss: 1.093818, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "838: [D loss: 0.644410, acc: 0.625000]  [A loss: 0.968366, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "839: [D loss: 0.618612, acc: 0.632812]  [A loss: 0.985311, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "840: [D loss: 0.582483, acc: 0.695312]  [A loss: 1.067256, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "841: [D loss: 0.610689, acc: 0.695312]  [A loss: 0.976988, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "842: [D loss: 0.605884, acc: 0.673828]  [A loss: 1.164746, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "843: [D loss: 0.621267, acc: 0.646484]  [A loss: 0.810538, acc: 0.421875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "844: [D loss: 0.611328, acc: 0.664062]  [A loss: 1.306422, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "845: [D loss: 0.642056, acc: 0.601562]  [A loss: 0.805390, acc: 0.394531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "846: [D loss: 0.662228, acc: 0.609375]  [A loss: 1.218431, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "847: [D loss: 0.635826, acc: 0.625000]  [A loss: 0.796332, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "848: [D loss: 0.620553, acc: 0.666016]  [A loss: 1.075469, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "849: [D loss: 0.577706, acc: 0.708984]  [A loss: 0.883370, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "850: [D loss: 0.580972, acc: 0.728516]  [A loss: 1.091644, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "851: [D loss: 0.626478, acc: 0.648438]  [A loss: 0.907737, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "852: [D loss: 0.644231, acc: 0.632812]  [A loss: 0.964262, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "853: [D loss: 0.600457, acc: 0.673828]  [A loss: 0.928682, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "854: [D loss: 0.639479, acc: 0.644531]  [A loss: 1.142143, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "855: [D loss: 0.606302, acc: 0.679688]  [A loss: 0.871296, acc: 0.343750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "856: [D loss: 0.617057, acc: 0.660156]  [A loss: 1.088293, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "857: [D loss: 0.600253, acc: 0.693359]  [A loss: 0.825650, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "858: [D loss: 0.612204, acc: 0.654297]  [A loss: 1.144914, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "859: [D loss: 0.624424, acc: 0.628906]  [A loss: 0.839648, acc: 0.363281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "860: [D loss: 0.596604, acc: 0.679688]  [A loss: 1.141804, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "861: [D loss: 0.607959, acc: 0.693359]  [A loss: 0.936107, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "862: [D loss: 0.611093, acc: 0.666016]  [A loss: 1.047105, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "863: [D loss: 0.601281, acc: 0.675781]  [A loss: 0.835497, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "864: [D loss: 0.595131, acc: 0.677734]  [A loss: 1.167685, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "865: [D loss: 0.596213, acc: 0.699219]  [A loss: 0.899808, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "866: [D loss: 0.630328, acc: 0.621094]  [A loss: 1.196344, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "867: [D loss: 0.595952, acc: 0.667969]  [A loss: 0.778406, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "868: [D loss: 0.603613, acc: 0.685547]  [A loss: 1.252699, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "869: [D loss: 0.596402, acc: 0.679688]  [A loss: 0.844288, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "870: [D loss: 0.620524, acc: 0.673828]  [A loss: 1.332892, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "871: [D loss: 0.613175, acc: 0.650391]  [A loss: 0.910870, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "872: [D loss: 0.627053, acc: 0.656250]  [A loss: 1.154614, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "873: [D loss: 0.615717, acc: 0.652344]  [A loss: 0.899646, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "874: [D loss: 0.598417, acc: 0.679688]  [A loss: 1.001770, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "875: [D loss: 0.574178, acc: 0.703125]  [A loss: 1.053849, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "876: [D loss: 0.631062, acc: 0.638672]  [A loss: 0.959783, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "877: [D loss: 0.582324, acc: 0.689453]  [A loss: 0.931451, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "878: [D loss: 0.640196, acc: 0.607422]  [A loss: 1.085212, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "879: [D loss: 0.605629, acc: 0.703125]  [A loss: 0.883721, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "880: [D loss: 0.597272, acc: 0.691406]  [A loss: 1.060378, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "881: [D loss: 0.576343, acc: 0.708984]  [A loss: 0.993732, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "882: [D loss: 0.555028, acc: 0.726562]  [A loss: 1.106056, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "883: [D loss: 0.609478, acc: 0.685547]  [A loss: 0.975971, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "884: [D loss: 0.596162, acc: 0.691406]  [A loss: 1.022292, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "885: [D loss: 0.550125, acc: 0.726562]  [A loss: 0.966600, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "886: [D loss: 0.625710, acc: 0.650391]  [A loss: 1.155175, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "887: [D loss: 0.610712, acc: 0.650391]  [A loss: 0.883335, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "888: [D loss: 0.601120, acc: 0.669922]  [A loss: 1.243421, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "889: [D loss: 0.570386, acc: 0.712891]  [A loss: 0.862453, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "890: [D loss: 0.607836, acc: 0.636719]  [A loss: 1.249510, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "891: [D loss: 0.566596, acc: 0.710938]  [A loss: 0.844046, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "892: [D loss: 0.641080, acc: 0.626953]  [A loss: 1.426471, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "893: [D loss: 0.645252, acc: 0.611328]  [A loss: 0.721112, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "894: [D loss: 0.647660, acc: 0.626953]  [A loss: 1.234435, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "895: [D loss: 0.647951, acc: 0.625000]  [A loss: 0.936819, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "896: [D loss: 0.620164, acc: 0.679688]  [A loss: 1.090029, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "897: [D loss: 0.618690, acc: 0.679688]  [A loss: 0.967634, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "898: [D loss: 0.589025, acc: 0.701172]  [A loss: 0.946694, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "899: [D loss: 0.603635, acc: 0.689453]  [A loss: 1.046725, acc: 0.183594]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "900: [D loss: 0.609632, acc: 0.642578]  [A loss: 0.900131, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "901: [D loss: 0.605662, acc: 0.671875]  [A loss: 1.086271, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "902: [D loss: 0.607389, acc: 0.693359]  [A loss: 0.992552, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "903: [D loss: 0.577356, acc: 0.720703]  [A loss: 1.069946, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "904: [D loss: 0.588661, acc: 0.669922]  [A loss: 0.879042, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "905: [D loss: 0.621598, acc: 0.673828]  [A loss: 1.328091, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "906: [D loss: 0.620022, acc: 0.640625]  [A loss: 0.780918, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "907: [D loss: 0.664830, acc: 0.605469]  [A loss: 1.211526, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "908: [D loss: 0.631120, acc: 0.632812]  [A loss: 0.798693, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "909: [D loss: 0.607085, acc: 0.689453]  [A loss: 1.201148, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "910: [D loss: 0.563234, acc: 0.722656]  [A loss: 0.959082, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "911: [D loss: 0.628731, acc: 0.669922]  [A loss: 1.078389, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "912: [D loss: 0.605908, acc: 0.667969]  [A loss: 0.984057, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "913: [D loss: 0.593491, acc: 0.697266]  [A loss: 1.060919, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "914: [D loss: 0.571566, acc: 0.697266]  [A loss: 0.952437, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "915: [D loss: 0.600481, acc: 0.673828]  [A loss: 1.095482, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "916: [D loss: 0.602520, acc: 0.679688]  [A loss: 0.938694, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "917: [D loss: 0.636170, acc: 0.644531]  [A loss: 1.188844, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "918: [D loss: 0.607862, acc: 0.656250]  [A loss: 0.780536, acc: 0.421875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "919: [D loss: 0.619739, acc: 0.644531]  [A loss: 1.195475, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "920: [D loss: 0.613615, acc: 0.658203]  [A loss: 0.865899, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "921: [D loss: 0.603193, acc: 0.660156]  [A loss: 1.203569, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "922: [D loss: 0.620220, acc: 0.638672]  [A loss: 0.917348, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "923: [D loss: 0.623777, acc: 0.658203]  [A loss: 1.184538, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "924: [D loss: 0.606344, acc: 0.687500]  [A loss: 0.873939, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "925: [D loss: 0.591065, acc: 0.685547]  [A loss: 1.133317, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "926: [D loss: 0.607801, acc: 0.662109]  [A loss: 0.897231, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "927: [D loss: 0.634707, acc: 0.654297]  [A loss: 1.093875, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "928: [D loss: 0.606673, acc: 0.669922]  [A loss: 0.922363, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "929: [D loss: 0.593706, acc: 0.679688]  [A loss: 1.160695, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "930: [D loss: 0.595803, acc: 0.701172]  [A loss: 0.916640, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "931: [D loss: 0.591401, acc: 0.681641]  [A loss: 1.101996, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "932: [D loss: 0.617044, acc: 0.666016]  [A loss: 0.837125, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "933: [D loss: 0.589188, acc: 0.695312]  [A loss: 1.208135, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "934: [D loss: 0.608187, acc: 0.679688]  [A loss: 0.850836, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "935: [D loss: 0.623602, acc: 0.644531]  [A loss: 1.023531, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "936: [D loss: 0.613045, acc: 0.636719]  [A loss: 0.917089, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "937: [D loss: 0.619633, acc: 0.667969]  [A loss: 1.021747, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "938: [D loss: 0.609927, acc: 0.642578]  [A loss: 0.976671, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "939: [D loss: 0.628080, acc: 0.630859]  [A loss: 1.178663, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "940: [D loss: 0.609667, acc: 0.658203]  [A loss: 0.890396, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "941: [D loss: 0.608730, acc: 0.703125]  [A loss: 1.167586, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "942: [D loss: 0.612736, acc: 0.656250]  [A loss: 0.787143, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "943: [D loss: 0.611873, acc: 0.650391]  [A loss: 1.247547, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "944: [D loss: 0.586636, acc: 0.707031]  [A loss: 0.790689, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "945: [D loss: 0.621640, acc: 0.656250]  [A loss: 1.227824, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "946: [D loss: 0.607036, acc: 0.685547]  [A loss: 0.841423, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "947: [D loss: 0.632777, acc: 0.650391]  [A loss: 1.211605, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "948: [D loss: 0.628481, acc: 0.638672]  [A loss: 0.788297, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "949: [D loss: 0.590142, acc: 0.666016]  [A loss: 1.119220, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "950: [D loss: 0.559306, acc: 0.712891]  [A loss: 0.918852, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "951: [D loss: 0.644574, acc: 0.603516]  [A loss: 1.131612, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "952: [D loss: 0.580894, acc: 0.703125]  [A loss: 0.846831, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "953: [D loss: 0.617335, acc: 0.662109]  [A loss: 1.219407, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "954: [D loss: 0.621665, acc: 0.660156]  [A loss: 0.854428, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "955: [D loss: 0.591143, acc: 0.707031]  [A loss: 1.088784, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "956: [D loss: 0.619257, acc: 0.673828]  [A loss: 0.881980, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "957: [D loss: 0.600742, acc: 0.677734]  [A loss: 1.153567, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "958: [D loss: 0.601964, acc: 0.652344]  [A loss: 1.052357, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "959: [D loss: 0.577907, acc: 0.699219]  [A loss: 1.011144, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "960: [D loss: 0.613811, acc: 0.687500]  [A loss: 1.110717, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "961: [D loss: 0.595830, acc: 0.675781]  [A loss: 0.969787, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "962: [D loss: 0.627666, acc: 0.644531]  [A loss: 1.080663, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "963: [D loss: 0.621669, acc: 0.660156]  [A loss: 0.986266, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "964: [D loss: 0.614417, acc: 0.666016]  [A loss: 0.940621, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "965: [D loss: 0.594987, acc: 0.681641]  [A loss: 1.006427, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "966: [D loss: 0.577158, acc: 0.691406]  [A loss: 0.980387, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "967: [D loss: 0.624823, acc: 0.662109]  [A loss: 1.118851, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "968: [D loss: 0.618241, acc: 0.669922]  [A loss: 0.846219, acc: 0.363281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "969: [D loss: 0.630858, acc: 0.650391]  [A loss: 1.299818, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "970: [D loss: 0.665958, acc: 0.601562]  [A loss: 0.750775, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "971: [D loss: 0.641162, acc: 0.632812]  [A loss: 1.151508, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "972: [D loss: 0.609939, acc: 0.636719]  [A loss: 0.892010, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "973: [D loss: 0.618302, acc: 0.634766]  [A loss: 1.063724, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "974: [D loss: 0.639273, acc: 0.638672]  [A loss: 0.846884, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "975: [D loss: 0.608037, acc: 0.669922]  [A loss: 1.003751, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "976: [D loss: 0.627409, acc: 0.662109]  [A loss: 0.920639, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "977: [D loss: 0.613710, acc: 0.664062]  [A loss: 1.076588, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "978: [D loss: 0.605204, acc: 0.683594]  [A loss: 0.877473, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "979: [D loss: 0.598132, acc: 0.687500]  [A loss: 1.131445, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "980: [D loss: 0.613839, acc: 0.671875]  [A loss: 0.905574, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "981: [D loss: 0.589264, acc: 0.691406]  [A loss: 1.093539, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "982: [D loss: 0.587669, acc: 0.691406]  [A loss: 0.996558, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "983: [D loss: 0.636705, acc: 0.664062]  [A loss: 1.046442, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "984: [D loss: 0.637104, acc: 0.646484]  [A loss: 0.969807, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "985: [D loss: 0.577394, acc: 0.716797]  [A loss: 0.916462, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "986: [D loss: 0.600169, acc: 0.656250]  [A loss: 1.122912, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "987: [D loss: 0.612618, acc: 0.656250]  [A loss: 0.862791, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "988: [D loss: 0.618563, acc: 0.656250]  [A loss: 1.260911, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "989: [D loss: 0.611734, acc: 0.646484]  [A loss: 0.844216, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "990: [D loss: 0.611740, acc: 0.673828]  [A loss: 1.121945, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "991: [D loss: 0.619490, acc: 0.664062]  [A loss: 0.835424, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "992: [D loss: 0.573293, acc: 0.703125]  [A loss: 1.122537, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "993: [D loss: 0.619317, acc: 0.658203]  [A loss: 0.952569, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "994: [D loss: 0.576073, acc: 0.707031]  [A loss: 0.968141, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "995: [D loss: 0.581810, acc: 0.707031]  [A loss: 0.984524, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "996: [D loss: 0.595974, acc: 0.685547]  [A loss: 1.132756, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "997: [D loss: 0.581053, acc: 0.691406]  [A loss: 1.055857, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "998: [D loss: 0.582539, acc: 0.697266]  [A loss: 1.009246, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "999: [D loss: 0.602513, acc: 0.669922]  [A loss: 1.137123, acc: 0.144531]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Elapsed: 3.835441239674886 min \n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100) \n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "**1. Explica qué hacen las siguientes líneas de código:**\n",
    "\n",
    "```python\n",
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100)\n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)\n",
    "```\n",
    "\n",
    "1. `timer = ElapsedTimer()`: Crea una instancia de la clase `ElapsedTimer` y la asigna a la variable `timer`. Esto inicia el temporizador para medir el tiempo transcurrido.\n",
    "\n",
    "2. `train(train_epochs=1000, batch_size=256, save_interval=100)`: Llama a la función train con los argumentos `train_epochs=1000, batch_size=256 y save_interval=100`. Esta función realiza el entrenamiento del modelo GAN durante 1000 épocas, utilizando un tamaño de lote de 256 y guardando imágenes generadas cada 100 épocas.\n",
    "\n",
    "3. `timer.elapsed_time()`: Imprime el tiempo transcurrido durante el entrenamiento de la GAN. Llama al método `elapsed_time` del objeto `timer` creado anteriormente, que muestra el tiempo transcurrido desde que se creó el objeto.\n",
    "\n",
    "4. `plot_images(fake=True)`: Genera y muestra imágenes falsas utilizando el generador de la GAN. Estas imágenes son generadas a partir de ruido aleatorio y se muestran en una cuadrícula.\n",
    "\n",
    "5. `plot_images(fake=False, saveToFile=True)`: Llama a la función plot_images con los argumentos `fake=False` y `saveToFile=True`. Esta función muestra imágenes reales del conjunto de datos original y también guarda una figura que contiene imágenes generadas y reales en un archivo.\n",
    "\n",
    "**2. Escribe el código necesario para mostrar las imágenes generadas en la última iteración y muestra los resultados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:36:36.139697Z",
     "iopub.status.busy": "2023-05-31T18:36:36.139310Z",
     "iopub.status.idle": "2023-05-31T18:36:36.982622Z",
     "shell.execute_reply": "2023-05-31T18:36:36.981527Z",
     "shell.execute_reply.started": "2023-05-31T18:36:36.139663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(saveToFile=False, fake=True, samples=16, noise=None, epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias consultadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:12:59.752261Z",
     "iopub.status.busy": "2023-05-31T09:12:59.751293Z",
     "iopub.status.idle": "2023-05-31T09:12:59.769764Z",
     "shell.execute_reply": "2023-05-31T09:12:59.768490Z",
     "shell.execute_reply.started": "2023-05-31T09:12:59.752219Z"
    }
   },
   "source": [
    "1. Distill: \"GANs and Generative Models\"\n",
    "   - Enlace: https://distill.pub/2016/deconv-checkerboard/\n",
    "2. Martin Arjovsky, Soumith Chintala, and Léon Bottou. \"Improved Techniques for Training GANs\"\n",
    "    - Enlace: https://papers.nips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf\n",
    "3. Machine Learning Mastery: \"A Gentle Introduction to Generative Adversarial Networks (GANs)\"\n",
    "   - Enlace: https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/ \n",
    "4. PyTorch: \"GAN Tutorials\"\n",
    "   - Enlace: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html \n",
    "5. TensorFlow: \"Generative Adversarial Networks\"\n",
    "    - Enlace: https://www.tensorflow.org/tutorials/generative/dcgan \n",
    "6. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. \"Deep Learning\"\n",
    "    - Enlace: http://www.deeplearningbook.org/\n",
    "7. François Chollet. \"Deep Learning with Python\"\n",
    "    - Enlace: https://www.manning.com/books/deep-learning-with-python\n",
    "8. Jordi de la Torre Gallart. \"Modelos Generativos\"\n",
    "    - Enlace: https://campus.uoc.edu/ (Material de la asignatura)\n",
    "9. Justin Johnson, Alexandre Alahi, Li Fei-Fei. \"Perceptual losses for real-time style transfer and super-resolution\"\n",
    "    - Enlace: https://arxiv.org/pdf/1603.08155.pdf\n",
    "10. Vincent Dumoulin, Francesco Visin. \"A guide to convolution arithmetic for deep learning\"\n",
    "    - Enlace: https://arxiv.org/pdf/1603.07285.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
