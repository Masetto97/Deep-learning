{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.975 · Deep Learning · PEC4</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Master universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informatica, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC 4: Modelos generativos\n",
    "\n",
    "En esta práctica implementaremos uno de los tipos de modelos generativos más utilizados actualmente, las redes generativas adversarias, ie. **GANs**.\n",
    "\n",
    "<u>Consideraciones generales</u>:\n",
    "\n",
    "- Esta PEC debe realizarse de manera **estrictamente individual**. Cualquier indicio de copia será penalizado con un suspenso (D) para todas las partes implicadas y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "- Es necesario que el estudiante indique **todas las fuentes** que ha utilizado para la realización de la PEC. Si no es así, se considerará que el estudiante ha cometido plagio, siendo penalizado con un suspenso (D) y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "\n",
    "<u>Formato de entrega</u>:\n",
    "\n",
    "- Algunos ejercicios pueden suponer varios minutos de ejecución, por lo que la entrega debe realizarse en **formato notebook** y en **formato html**, donde se vea el código, los resultados y comentarios de cada ejercicio. Se puede exportar el notebook a HTML desde el menú File $\\to$ Download as $\\to$ HTML.\n",
    "- Existe un tipo de celda especial para albergar texto. Este tipo de celda le será muy útil para responder a las diferentes preguntas teóricas planteadas a lo largo de la actividad. Puede cambiar el tipo de celda a este tipo, en el menú: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introducción\n",
    "\n",
    "El objetivo de esta PEC es comprender la implementación de una solución generativa, utilizando DCGANs para la generación de imágenes, mediante el conjunto de datos de referencia en deep learning más sencillo existente: MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:17:59.165823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 15:17:59.329596: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-24 15:18:00.174514: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64\n",
      "2023-05-24 15:18:00.174690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64\n",
      "2023-05-24 15:18:00.174695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jordi/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obtención de los datos\n",
    "\n",
    "El código para cargar los datos es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "img_channels = 1\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1,5 pts.]:</strong> \n",
    "Añade un comentario explicativo, a cada una de las líneas de código de abajo, indicando cuál es su funcionalidad.</div>\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "* `latent_dim = 100`: se define un espacio latente formado por un vector de 100 dimensiones que servirá al generador para generar las imágenes falsas.\n",
    "* `img_rows, img_cols = 28, 28`: dimensiones de las imágenes   \n",
    "* `img_channels = 1`: número de canales de la imagen (monocromo)\n",
    "* `(x_train, _), (_, _) = mnist.load_data()`: función que permite cargar el conjunto de datos\n",
    "* `x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)`: Se añade la dimensión de los canales, que siendo de 1 es necesaria para la utilización de las librerías tensoriales.\n",
    "* `x_train = x_train.astype('float32')`: los datos originales son enteros de 1 byte, se convierten en reales de 32 bits para poder realizar el entrenamiento/optimización\n",
    "* `x_train /= 255`: Normalización de los datos en el intervalo [0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementación del Generador\n",
    "\n",
    "A continuación se muestra una propuesta de generador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(): \n",
    "    dropout = 0.4\n",
    "    depth = 256 # 64+64+64+64\n",
    "    dim = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 100\n",
    "    # Out: dim x dim x depth\n",
    "    model.add(Dense(dim*dim*depth, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((dim, dim, depth)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # In: dim x dim x depth\n",
    "    # Out: 2*dim x 2*dim x depth/2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1,75 pts.]:</strong> \n",
    "Contesta a las preguntas siguientes:\n",
    "</div>\n",
    "\n",
    "**1. ¿Cuál es la finalidad del generador?:**\n",
    "\n",
    "El generador sintetiza nuevas imágenes a partir de un ruido de 100 dimensiones (distribución uniforme entre -1,0 y 1,0) utilizando la inversa de la convolución, llamada convolución transpuesta.\n",
    "\n",
    "**2. Investigar por qué se utiliza `Upsampling` en las dos primeras capas en lugar de la `Conv2DTranspose` propuesta en DCGAN. Dar una justificación:**\n",
    "\n",
    "En vez de la convolución de pasos fraccionados, tal y como se sugiere en DCGAN, se utiliza el muestreo superior entre las tres primeras capas, sintetizando imágenes de escritura a mano más realistas.\n",
    "\n",
    "**3. ¿Por qué se utiliza la normalización entre capas?**\n",
    "\n",
    "Entre capas, la normalización por lotes estabiliza el aprendizaje.\n",
    "\n",
    "**4. ¿Qué funciones de activación se utilizan? ¿Cuál es la razón de la sigmoide en la última capa?**\n",
    "\n",
    "La función de activación después de cada capa es una ReLU. La salida del sigmoide en la última capa produce la imagen falsa. La razón de la sigmoide en la última capa se asegura que la salida esté en el intervalo [0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación del Discriminador\n",
    "\n",
    "A continuación se muestra el discriminador propuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (W−F+2P)/S+1\n",
    "def discriminator_model():\n",
    "    depth = 64\n",
    "    dropout = 0.4\n",
    "    input_shape = (img_rows, img_cols, img_channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 28 x 28 x 1, depth = 1\n",
    "    # Out: 14 x 14 x 1, depth=64\n",
    "    model.add(Conv2D(depth, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Out: 1-dim probability\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Exercici [1,75 pts.]:</strong> \n",
    "Contesta a las preguntas siguientes:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Cuál es la finalidad del discriminador?:**\n",
    "\n",
    "Un discriminador que indica lo real que es una imagen, es básicamente una red neuronal convolucional profunda.\n",
    "\n",
    "La función de activación utilizada en cada capa de CNN es una ReLU con fugas.\n",
    " \n",
    "\n",
    "**2. ¿Cuáles son las dimensiones de los tensores y características de las variables de entrada y salida del discriminador? :**\n",
    "\n",
    "Para el conjunto de datos MNIST, la entrada es una imagen (28 píxeles x 28 píxeles x 1 canal). La salida sigmoide es un valor escalar de la probabilidad de la realidad de la imagen (0,0 es ciertamente falso, 1,0 es real, cualquier cosa que hay en medio es un área gris).\n",
    "\n",
    "**3. ¿Cuál es la diferencia con una CNN habitual?**\n",
    "\n",
    "La diferencia con una CNN típica es la ausencia de max-pooling entre capas. En su lugar, se utiliza una convolución con stride para reducir la dimensionalidad.\n",
    "\n",
    "**4. ¿Qué funciones de activación se utilizan?**\n",
    "\n",
    "La función de activación utilizada en cada capa de CNN es una ReLU con fugas.\n",
    "\n",
    "**5. ¿Cuál es la finalidad del dropout que encontramos en las capas?**\n",
    "\n",
    "Dropout 0,4 y 0,7 entre capas evita el ajuste excesivo y la memorización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelo GAN\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1 pts.]:</strong> \n",
    "Contesta a les siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿A qué llamamos modelo GAN y por qué recibe ese nombre?:**\n",
    "\n",
    "Llamamos GAN al conjunto de Generador y Discriminador. La denominación \"Generative Adversarial Network\" se refiere a la interacción competitiva entre el generador y el discriminador en el proceso de aprendizaje de la red. El generador es responsable de crear nuevas muestras de datos, como imágenes o textos, a partir de un conjunto de datos de entrada. Por su parte, el discriminador tiene como tarea distinguir entre las muestras generadas por el generador y las muestras reales del conjunto de datos original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Modelo Discriminador\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Qué función de pérdida utiliza el discriminador? ¿Por qué? :**\n",
    "\n",
    "Dado que la salida del discriminador es sigmoide, utilizamos `binary cross entropy` para la pérdida.\n",
    "\n",
    "**2. Busca en la bibliografía la razón por la que se propone utilizar `RMSProp` como optimizador en vez de otros.**\n",
    "RMSProp como optimizador genera imágenes falsas más realistas en comparación con Adam para este caso.\n",
    "\n",
    "**3. ¿Cuál es la razón de utilizar decay?**\n",
    "\n",
    "El weight decay y el valor del clip estabilizan el aprendizaje durante la última parte del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:18:41.395272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.439752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.439913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.440774: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 15:18:41.442154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.442317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.442442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.999598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:41.999937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:42.000059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 15:18:42.000174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2105 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/jordi/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator_model()\n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "                      optimizer=RMSprop(lr=0.0002, decay=6e-8), \n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Modelo adversario\n",
    "\n",
    "El modelo adversario es sólo el generador-discriminador apilados juntos. Los parámetros de entrenamiento son los mismos que en el modelo Discriminador, salvo por una tasa de aprendizaje reducida y la correspondiente disminución del peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_model():\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=RMSprop(lr=0.0001, decay=3e-8), \n",
    "                  metrics=['accuracy'])\n",
    "    discriminator.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial = adversarial_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(saveToFile=False, fake=True, samples=16, noise=None, epoch=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, latent_dim])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % epoch\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [img_rows, img_cols])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if saveToFile:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero determinamos si el modelo de discriminador es correcto entrenándolo solo con imágenes reales y falsas. Después, los modelos Discriminador y Adversario entrenan uno tras otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_epochs=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_dim])\n",
    "        for epoch in range(train_epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # select a random half of images\n",
    "            images_train = x_train[np.random.randint(0, x_train.shape[0], size=batch_size), :, :, :]\n",
    "            \n",
    "            # sample noise and generate a batch of new images\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
    "            images_fake = generator.predict(noise)\n",
    "            \n",
    "            # train the discriminator (real classified as ones and generated as zeros)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = discriminator.train_on_batch(x, y)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            \n",
    "            # train the generator (wants discriminator to mistake images as real)\n",
    "            y = np.ones([batch_size, 1])\n",
    "            a_loss = adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            log_msg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "            log_msg = \"%s  [A loss: %f, acc: %f]\" % (log_msg, a_loss[0], a_loss[1])\n",
    "            print(log_msg)\n",
    "            if save_interval>0:\n",
    "                if (epoch+1)%save_interval==0:\n",
    "                    plot_images(saveToFile=True, samples=noise_input.shape[0],\n",
    "                                noise=noise_input, epoch=(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [2 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Cuál es la finalidad de `noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])`? ¿Por qué estas dimensiones?**\n",
    "\n",
    "La finalidad de esta línea es la generación de ruido aleatorio que servirá como punto de partida para que el generador genere una imagen. La primera de las dimensiones es el tamaño del `batch`, la segunda las dimensiones del vector de variables latentes.\n",
    "\n",
    "**2. ¿Cuál es la finalidad de `images_fake = generator.predict(noise)`?**\n",
    "\n",
    "Generar las imágenes fake por un batch a partir del generador.\n",
    "\n",
    "**3. ¿Cuál es la finalidad del código que sigue?**\n",
    "```python\n",
    "x = np.concatenate((images_train, images_fake))\n",
    "y = np.ondas([2*batch_size, 1])\n",
    "y[batch_size:, :] = 0\n",
    "```\n",
    "\n",
    "Su finalidad es preparar un conjunto formado por imágenes reales e imágenes fake. En las fake se las etiqueta como 0, mientras que en las reales como 1. Esta información se utilizará para entrenar al modelo discriminador.\n",
    "\n",
    "**4. ¿Qué realiza el comando `d_loss = discriminator.train_on_batch(x, y)`? ¿Qué devuelve?**\n",
    "\n",
    "Este pedido ejecuta el entrenamiento del discriminador. Devuelve la función de pérdida asociada al error de entrenamiento.\n",
    "\n",
    "**5. ¿Qué realiza el comando `a_loss = adversarial.train_on_batch(noise, y)`? ¿Qué devuelve?**\n",
    "\n",
    "Este pedido realiza el entrenamiento del generador. Devuelve la función de pérdida asociada al error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:19:04.848117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-24 15:19:05.041267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:19:06.961209: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-24 15:19:07.413763: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-24 15:19:07.413792: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-24 15:19:07.566421: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-24 15:19:07.566448: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-24 15:19:09.580712: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/sequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.693025, acc: 0.501953]  [A loss: 1.277778, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "1: [D loss: 0.590089, acc: 0.500000]  [A loss: 1.795848, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "2: [D loss: 0.344943, acc: 0.966797]  [A loss: 1.040250, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "3: [D loss: 0.105615, acc: 1.000000]  [A loss: 12.553440, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "4: [D loss: 1.348715, acc: 0.507812]  [A loss: 0.314578, acc: 0.972656]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "5: [D loss: 0.069144, acc: 0.998047]  [A loss: 0.507411, acc: 0.789062]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "6: [D loss: 0.045524, acc: 0.992188]  [A loss: 0.234772, acc: 0.980469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "7: [D loss: 0.044444, acc: 0.998047]  [A loss: 0.191159, acc: 0.972656]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8: [D loss: 0.047138, acc: 0.996094]  [A loss: 0.124217, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "9: [D loss: 0.055881, acc: 0.994141]  [A loss: 0.111494, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "10: [D loss: 0.053900, acc: 0.996094]  [A loss: 0.148725, acc: 0.980469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "11: [D loss: 0.053275, acc: 0.998047]  [A loss: 0.126176, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "12: [D loss: 0.059214, acc: 1.000000]  [A loss: 0.095839, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "13: [D loss: 0.060268, acc: 1.000000]  [A loss: 0.112548, acc: 0.980469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "14: [D loss: 0.060562, acc: 0.998047]  [A loss: 0.062085, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "15: [D loss: 0.053953, acc: 1.000000]  [A loss: 0.120061, acc: 0.988281]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "16: [D loss: 0.067136, acc: 0.992188]  [A loss: 0.014873, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "17: [D loss: 0.050497, acc: 1.000000]  [A loss: 0.085054, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "18: [D loss: 0.043249, acc: 1.000000]  [A loss: 0.084518, acc: 0.976562]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "19: [D loss: 0.043178, acc: 1.000000]  [A loss: 0.035758, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "20: [D loss: 0.039534, acc: 0.996094]  [A loss: 0.005022, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "21: [D loss: 0.032665, acc: 0.998047]  [A loss: 0.003716, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "22: [D loss: 0.024465, acc: 0.998047]  [A loss: 0.003414, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "23: [D loss: 0.019602, acc: 1.000000]  [A loss: 0.005991, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "24: [D loss: 0.014463, acc: 1.000000]  [A loss: 0.028536, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "25: [D loss: 0.014831, acc: 0.998047]  [A loss: 0.011591, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "26: [D loss: 0.012238, acc: 1.000000]  [A loss: 0.029597, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "27: [D loss: 0.011521, acc: 1.000000]  [A loss: 0.021978, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "28: [D loss: 0.013129, acc: 0.998047]  [A loss: 0.009115, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "29: [D loss: 0.008877, acc: 1.000000]  [A loss: 0.030458, acc: 0.992188]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "30: [D loss: 0.013753, acc: 0.998047]  [A loss: 0.001394, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "31: [D loss: 0.008943, acc: 1.000000]  [A loss: 0.031312, acc: 0.984375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "32: [D loss: 0.008798, acc: 0.998047]  [A loss: 0.003501, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "33: [D loss: 0.007213, acc: 1.000000]  [A loss: 0.016410, acc: 0.996094]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "34: [D loss: 0.006023, acc: 1.000000]  [A loss: 0.052310, acc: 0.988281]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "35: [D loss: 0.006690, acc: 1.000000]  [A loss: 0.029740, acc: 0.984375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "36: [D loss: 0.011253, acc: 0.998047]  [A loss: 0.000110, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "37: [D loss: 0.013024, acc: 1.000000]  [A loss: 1.639425, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "38: [D loss: 0.023979, acc: 0.996094]  [A loss: 0.000001, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "39: [D loss: 0.120160, acc: 0.964844]  [A loss: 38.362797, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "40: [D loss: 5.591490, acc: 0.500000]  [A loss: 0.000002, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "41: [D loss: 0.256635, acc: 0.880859]  [A loss: 0.003043, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "42: [D loss: 0.083596, acc: 0.992188]  [A loss: 0.077046, acc: 0.976562]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "43: [D loss: 0.074121, acc: 0.994141]  [A loss: 0.297328, acc: 0.863281]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "44: [D loss: 0.115831, acc: 0.972656]  [A loss: 2.163208, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "45: [D loss: 0.111861, acc: 0.982422]  [A loss: 4.719987, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "46: [D loss: 0.100073, acc: 0.988281]  [A loss: 3.415238, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "47: [D loss: 0.107273, acc: 0.990234]  [A loss: 6.068890, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "48: [D loss: 0.101310, acc: 0.984375]  [A loss: 1.718512, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "49: [D loss: 0.247724, acc: 0.892578]  [A loss: 16.797813, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "50: [D loss: 1.691124, acc: 0.500000]  [A loss: 0.008932, acc: 1.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "51: [D loss: 1.545516, acc: 0.509766]  [A loss: 11.693152, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "52: [D loss: 0.850487, acc: 0.576172]  [A loss: 3.879872, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "53: [D loss: 0.082540, acc: 0.998047]  [A loss: 4.656926, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "54: [D loss: 0.060473, acc: 0.996094]  [A loss: 4.783625, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "55: [D loss: 0.056920, acc: 0.996094]  [A loss: 4.678383, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "56: [D loss: 0.054906, acc: 0.996094]  [A loss: 4.800716, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "57: [D loss: 0.054688, acc: 0.996094]  [A loss: 4.890267, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "58: [D loss: 0.041214, acc: 1.000000]  [A loss: 4.895052, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "59: [D loss: 0.040572, acc: 0.996094]  [A loss: 4.785220, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "60: [D loss: 0.053252, acc: 0.994141]  [A loss: 4.888289, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "61: [D loss: 0.041309, acc: 0.998047]  [A loss: 4.993505, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "62: [D loss: 0.042703, acc: 0.994141]  [A loss: 4.964834, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "63: [D loss: 0.030398, acc: 1.000000]  [A loss: 5.420620, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "64: [D loss: 0.029851, acc: 0.998047]  [A loss: 5.288746, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "65: [D loss: 0.038133, acc: 0.998047]  [A loss: 5.497266, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "66: [D loss: 0.036084, acc: 0.994141]  [A loss: 4.805020, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step\n",
      "67: [D loss: 0.034028, acc: 0.998047]  [A loss: 6.141680, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "68: [D loss: 0.032775, acc: 0.996094]  [A loss: 4.763631, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "69: [D loss: 0.034047, acc: 0.996094]  [A loss: 5.340449, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "70: [D loss: 0.024694, acc: 0.996094]  [A loss: 4.826488, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "71: [D loss: 0.032638, acc: 0.994141]  [A loss: 5.696912, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "72: [D loss: 0.029583, acc: 0.992188]  [A loss: 4.985974, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "73: [D loss: 0.037067, acc: 0.998047]  [A loss: 6.816721, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "74: [D loss: 0.057419, acc: 0.982422]  [A loss: 2.370356, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "75: [D loss: 0.405978, acc: 0.750000]  [A loss: 20.189262, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "76: [D loss: 3.430904, acc: 0.501953]  [A loss: 4.400660, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "77: [D loss: 0.062130, acc: 1.000000]  [A loss: 4.340017, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "78: [D loss: 0.069901, acc: 0.996094]  [A loss: 4.445925, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "79: [D loss: 0.083107, acc: 0.994141]  [A loss: 4.264496, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "80: [D loss: 0.079953, acc: 0.994141]  [A loss: 4.564996, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "81: [D loss: 0.085585, acc: 0.986328]  [A loss: 4.173936, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "82: [D loss: 0.129255, acc: 0.980469]  [A loss: 4.071173, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "83: [D loss: 0.103082, acc: 0.988281]  [A loss: 4.134393, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "84: [D loss: 0.112922, acc: 0.986328]  [A loss: 4.210994, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "85: [D loss: 0.120962, acc: 0.974609]  [A loss: 3.670797, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "86: [D loss: 0.135635, acc: 0.964844]  [A loss: 4.689451, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "87: [D loss: 0.223324, acc: 0.919922]  [A loss: 1.396870, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "88: [D loss: 0.443531, acc: 0.759766]  [A loss: 8.295458, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "89: [D loss: 1.608835, acc: 0.544922]  [A loss: 0.970108, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "90: [D loss: 0.551484, acc: 0.656250]  [A loss: 3.587179, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "91: [D loss: 0.220648, acc: 0.929688]  [A loss: 2.212676, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "92: [D loss: 0.161055, acc: 0.972656]  [A loss: 2.704684, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "93: [D loss: 0.159514, acc: 0.966797]  [A loss: 2.418490, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "94: [D loss: 0.147605, acc: 0.970703]  [A loss: 2.652374, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "95: [D loss: 0.144323, acc: 0.976562]  [A loss: 2.529535, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "96: [D loss: 0.164417, acc: 0.972656]  [A loss: 2.557911, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "97: [D loss: 0.166633, acc: 0.966797]  [A loss: 2.422219, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "98: [D loss: 0.177713, acc: 0.953125]  [A loss: 3.358704, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "99: [D loss: 0.220349, acc: 0.931641]  [A loss: 1.246442, acc: 0.261719]\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "100: [D loss: 0.429610, acc: 0.777344]  [A loss: 6.287898, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "101: [D loss: 1.303507, acc: 0.568359]  [A loss: 0.649630, acc: 0.589844]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "102: [D loss: 0.619118, acc: 0.593750]  [A loss: 2.919254, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "103: [D loss: 0.331745, acc: 0.896484]  [A loss: 1.584570, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "104: [D loss: 0.227371, acc: 0.951172]  [A loss: 1.992582, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "105: [D loss: 0.234261, acc: 0.951172]  [A loss: 1.764352, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "106: [D loss: 0.208931, acc: 0.958984]  [A loss: 2.096911, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "107: [D loss: 0.203241, acc: 0.955078]  [A loss: 1.914024, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "108: [D loss: 0.194552, acc: 0.976562]  [A loss: 2.005836, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "109: [D loss: 0.201840, acc: 0.955078]  [A loss: 2.006894, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "110: [D loss: 0.212248, acc: 0.955078]  [A loss: 1.941235, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "111: [D loss: 0.193187, acc: 0.964844]  [A loss: 2.329437, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "112: [D loss: 0.171705, acc: 0.966797]  [A loss: 1.720507, acc: 0.031250]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "113: [D loss: 0.238518, acc: 0.925781]  [A loss: 3.681925, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "114: [D loss: 0.449881, acc: 0.812500]  [A loss: 0.170227, acc: 0.980469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "115: [D loss: 1.292349, acc: 0.503906]  [A loss: 3.896605, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "116: [D loss: 0.733722, acc: 0.652344]  [A loss: 1.035497, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "117: [D loss: 0.372359, acc: 0.796875]  [A loss: 1.884835, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "118: [D loss: 0.265756, acc: 0.945312]  [A loss: 1.583940, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "119: [D loss: 0.270383, acc: 0.951172]  [A loss: 1.807500, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "120: [D loss: 0.242858, acc: 0.966797]  [A loss: 1.763886, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "121: [D loss: 0.234339, acc: 0.960938]  [A loss: 1.793223, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "122: [D loss: 0.248673, acc: 0.957031]  [A loss: 1.691460, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "123: [D loss: 0.255818, acc: 0.943359]  [A loss: 1.843430, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "124: [D loss: 0.251836, acc: 0.943359]  [A loss: 1.578102, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "125: [D loss: 0.248597, acc: 0.943359]  [A loss: 2.835409, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "126: [D loss: 0.295651, acc: 0.908203]  [A loss: 0.650760, acc: 0.609375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "127: [D loss: 0.571798, acc: 0.605469]  [A loss: 4.607463, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "128: [D loss: 1.123258, acc: 0.541016]  [A loss: 0.544880, acc: 0.750000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "129: [D loss: 0.626296, acc: 0.542969]  [A loss: 1.734046, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "130: [D loss: 0.319753, acc: 0.923828]  [A loss: 1.358520, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "131: [D loss: 0.321378, acc: 0.933594]  [A loss: 1.530821, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132: [D loss: 0.286841, acc: 0.953125]  [A loss: 1.532036, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "133: [D loss: 0.265263, acc: 0.964844]  [A loss: 1.499036, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "134: [D loss: 0.266981, acc: 0.957031]  [A loss: 1.702957, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "135: [D loss: 0.284436, acc: 0.935547]  [A loss: 1.270456, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "136: [D loss: 0.288526, acc: 0.923828]  [A loss: 2.375799, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "137: [D loss: 0.347714, acc: 0.894531]  [A loss: 0.626800, acc: 0.656250]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "138: [D loss: 0.537502, acc: 0.605469]  [A loss: 3.457209, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "139: [D loss: 0.801939, acc: 0.605469]  [A loss: 0.503103, acc: 0.804688]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "140: [D loss: 0.567895, acc: 0.572266]  [A loss: 1.722653, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "141: [D loss: 0.322157, acc: 0.912109]  [A loss: 1.225510, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "142: [D loss: 0.323077, acc: 0.927734]  [A loss: 1.580796, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "143: [D loss: 0.303840, acc: 0.941406]  [A loss: 1.279955, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "144: [D loss: 0.287041, acc: 0.939453]  [A loss: 1.585421, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "145: [D loss: 0.326374, acc: 0.927734]  [A loss: 1.295036, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "146: [D loss: 0.296429, acc: 0.921875]  [A loss: 2.008469, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "147: [D loss: 0.298209, acc: 0.925781]  [A loss: 1.194395, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "148: [D loss: 0.326708, acc: 0.867188]  [A loss: 2.877871, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "149: [D loss: 0.492918, acc: 0.761719]  [A loss: 0.230424, acc: 0.984375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "150: [D loss: 1.057172, acc: 0.501953]  [A loss: 2.886345, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "151: [D loss: 0.645970, acc: 0.611328]  [A loss: 0.794605, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "152: [D loss: 0.486975, acc: 0.664062]  [A loss: 1.648582, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "153: [D loss: 0.371301, acc: 0.898438]  [A loss: 1.181253, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "154: [D loss: 0.366631, acc: 0.896484]  [A loss: 1.621659, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "155: [D loss: 0.360013, acc: 0.910156]  [A loss: 1.292816, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "156: [D loss: 0.393495, acc: 0.861328]  [A loss: 1.851289, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "157: [D loss: 0.442347, acc: 0.839844]  [A loss: 0.971706, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "158: [D loss: 0.451439, acc: 0.742188]  [A loss: 2.599130, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "159: [D loss: 0.585257, acc: 0.667969]  [A loss: 0.428619, acc: 0.882812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "160: [D loss: 0.728959, acc: 0.527344]  [A loss: 2.229122, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "161: [D loss: 0.549242, acc: 0.703125]  [A loss: 0.818490, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "162: [D loss: 0.476109, acc: 0.697266]  [A loss: 1.615602, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "163: [D loss: 0.409938, acc: 0.863281]  [A loss: 1.074997, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "164: [D loss: 0.386106, acc: 0.876953]  [A loss: 1.648165, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "165: [D loss: 0.376943, acc: 0.890625]  [A loss: 1.044905, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "166: [D loss: 0.410073, acc: 0.822266]  [A loss: 2.076259, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "167: [D loss: 0.469930, acc: 0.765625]  [A loss: 0.590362, acc: 0.671875]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "168: [D loss: 0.608540, acc: 0.578125]  [A loss: 2.525495, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "169: [D loss: 0.632341, acc: 0.603516]  [A loss: 0.702373, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "170: [D loss: 0.507689, acc: 0.646484]  [A loss: 1.630117, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "171: [D loss: 0.408595, acc: 0.873047]  [A loss: 1.065906, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "172: [D loss: 0.395832, acc: 0.867188]  [A loss: 1.526163, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "173: [D loss: 0.385385, acc: 0.914062]  [A loss: 1.189504, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "174: [D loss: 0.391482, acc: 0.867188]  [A loss: 1.870001, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "175: [D loss: 0.412105, acc: 0.843750]  [A loss: 0.739123, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "176: [D loss: 0.527401, acc: 0.667969]  [A loss: 2.933310, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "177: [D loss: 0.782320, acc: 0.544922]  [A loss: 0.476714, acc: 0.832031]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "178: [D loss: 0.640768, acc: 0.529297]  [A loss: 1.783583, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "179: [D loss: 0.494577, acc: 0.738281]  [A loss: 0.942663, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "180: [D loss: 0.449756, acc: 0.783203]  [A loss: 1.564887, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "181: [D loss: 0.427822, acc: 0.878906]  [A loss: 0.946559, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "182: [D loss: 0.433236, acc: 0.796875]  [A loss: 1.802941, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "183: [D loss: 0.471807, acc: 0.779297]  [A loss: 0.743421, acc: 0.500000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "184: [D loss: 0.509186, acc: 0.689453]  [A loss: 2.135763, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "185: [D loss: 0.575805, acc: 0.632812]  [A loss: 0.613897, acc: 0.683594]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "186: [D loss: 0.540617, acc: 0.613281]  [A loss: 1.793529, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "187: [D loss: 0.478302, acc: 0.736328]  [A loss: 0.827296, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "188: [D loss: 0.451342, acc: 0.753906]  [A loss: 1.751025, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "189: [D loss: 0.459101, acc: 0.789062]  [A loss: 0.691449, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "190: [D loss: 0.513960, acc: 0.683594]  [A loss: 2.026627, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "191: [D loss: 0.549198, acc: 0.656250]  [A loss: 0.663526, acc: 0.585938]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "192: [D loss: 0.500022, acc: 0.658203]  [A loss: 1.750140, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "193: [D loss: 0.494676, acc: 0.755859]  [A loss: 0.732063, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "194: [D loss: 0.471488, acc: 0.722656]  [A loss: 1.798009, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "195: [D loss: 0.480969, acc: 0.746094]  [A loss: 0.702198, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "196: [D loss: 0.494091, acc: 0.714844]  [A loss: 1.841629, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "197: [D loss: 0.520097, acc: 0.724609]  [A loss: 0.695574, acc: 0.562500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step\n",
      "198: [D loss: 0.547146, acc: 0.644531]  [A loss: 1.854545, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "199: [D loss: 0.533715, acc: 0.671875]  [A loss: 0.656968, acc: 0.605469]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "200: [D loss: 0.525931, acc: 0.658203]  [A loss: 1.730413, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "201: [D loss: 0.530167, acc: 0.705078]  [A loss: 0.728337, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "202: [D loss: 0.524917, acc: 0.666016]  [A loss: 1.702221, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "203: [D loss: 0.520530, acc: 0.714844]  [A loss: 0.738521, acc: 0.535156]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "204: [D loss: 0.529219, acc: 0.667969]  [A loss: 1.707672, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "205: [D loss: 0.531775, acc: 0.726562]  [A loss: 0.746122, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "206: [D loss: 0.526075, acc: 0.656250]  [A loss: 1.786262, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "207: [D loss: 0.551644, acc: 0.697266]  [A loss: 0.719988, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "208: [D loss: 0.557907, acc: 0.632812]  [A loss: 1.777744, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "209: [D loss: 0.574207, acc: 0.664062]  [A loss: 0.714573, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "210: [D loss: 0.550737, acc: 0.675781]  [A loss: 1.679415, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "211: [D loss: 0.478007, acc: 0.791016]  [A loss: 0.810331, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "212: [D loss: 0.566718, acc: 0.660156]  [A loss: 1.789580, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "213: [D loss: 0.607591, acc: 0.640625]  [A loss: 0.669547, acc: 0.582031]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "214: [D loss: 0.567789, acc: 0.640625]  [A loss: 1.554428, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "215: [D loss: 0.550563, acc: 0.710938]  [A loss: 0.781890, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "216: [D loss: 0.516318, acc: 0.679688]  [A loss: 1.564447, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "217: [D loss: 0.521582, acc: 0.750000]  [A loss: 0.730079, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "218: [D loss: 0.538318, acc: 0.673828]  [A loss: 1.605234, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "219: [D loss: 0.596973, acc: 0.654297]  [A loss: 0.681595, acc: 0.578125]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "220: [D loss: 0.537591, acc: 0.644531]  [A loss: 1.644783, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "221: [D loss: 0.490958, acc: 0.771484]  [A loss: 0.799293, acc: 0.414062]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "222: [D loss: 0.552971, acc: 0.642578]  [A loss: 1.917850, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "223: [D loss: 0.646061, acc: 0.595703]  [A loss: 0.682659, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100) \n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<p><strong>Ejercicio [1 pts.]:</strong> \n",
    "Contesta a las siguientes preguntas:\n",
    "</div>\n",
    "\n",
    "**1. Explica qué hacen las siguientes líneas de código: **\n",
    "\n",
    "```python\n",
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100)\n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)\n",
    "```\n",
    "\n",
    "Ponen en marcha un timer para contabilizar el tiempo total de entrenamiento, llevan a cabo el entrenamiento y finalmente guardan unas muestras de imágenes generadas y reales para ver si el modelo ha hecho su función.\n",
    "\n",
    "**2. Escribe el código necesario para mostrar las imágenes generadas en la última iteración y muestra los resultados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias consultadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Añadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
